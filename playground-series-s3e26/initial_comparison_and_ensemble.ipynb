{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualisation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MaxAbsScaler, PowerTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import clone as clone_model\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.utils import compute_sample_weight, compute_class_weight\n",
    "\n",
    "\n",
    "# others\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "import catboost as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>N_Days</th>\n",
       "      <th>Drug</th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ascites</th>\n",
       "      <th>Hepatomegaly</th>\n",
       "      <th>Spiders</th>\n",
       "      <th>Edema</th>\n",
       "      <th>Bilirubin</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Albumin</th>\n",
       "      <th>Copper</th>\n",
       "      <th>Alk_Phos</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>Tryglicerides</th>\n",
       "      <th>Platelets</th>\n",
       "      <th>Prothrombin</th>\n",
       "      <th>Stage</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>999</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>21532</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>2.3</td>\n",
       "      <td>316.0</td>\n",
       "      <td>3.35</td>\n",
       "      <td>172.0</td>\n",
       "      <td>1601.0</td>\n",
       "      <td>179.80</td>\n",
       "      <td>63.0</td>\n",
       "      <td>394.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2574</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>19237</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>364.0</td>\n",
       "      <td>3.54</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>134.85</td>\n",
       "      <td>88.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3428</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>13727</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>3.3</td>\n",
       "      <td>299.0</td>\n",
       "      <td>3.55</td>\n",
       "      <td>131.0</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>119.35</td>\n",
       "      <td>50.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>11.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2576</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>18460</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.6</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.50</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1653.0</td>\n",
       "      <td>71.30</td>\n",
       "      <td>96.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>10.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>788</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>16658</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>1.1</td>\n",
       "      <td>346.0</td>\n",
       "      <td>3.65</td>\n",
       "      <td>63.0</td>\n",
       "      <td>1181.0</td>\n",
       "      <td>125.55</td>\n",
       "      <td>96.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7900</th>\n",
       "      <td>1166</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>16839</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.8</td>\n",
       "      <td>309.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1629.0</td>\n",
       "      <td>79.05</td>\n",
       "      <td>224.0</td>\n",
       "      <td>344.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7901</th>\n",
       "      <td>1492</td>\n",
       "      <td>Placebo</td>\n",
       "      <td>17031</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.9</td>\n",
       "      <td>260.0</td>\n",
       "      <td>3.43</td>\n",
       "      <td>62.0</td>\n",
       "      <td>1440.0</td>\n",
       "      <td>142.00</td>\n",
       "      <td>78.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7902</th>\n",
       "      <td>1576</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>25873</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>S</td>\n",
       "      <td>2.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>3.19</td>\n",
       "      <td>51.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>69.75</td>\n",
       "      <td>62.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>12.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7903</th>\n",
       "      <td>3584</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>22960</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.7</td>\n",
       "      <td>248.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1003.0</td>\n",
       "      <td>57.35</td>\n",
       "      <td>118.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7904</th>\n",
       "      <td>1978</td>\n",
       "      <td>D-penicillamine</td>\n",
       "      <td>19237</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0.7</td>\n",
       "      <td>256.0</td>\n",
       "      <td>3.23</td>\n",
       "      <td>22.0</td>\n",
       "      <td>645.0</td>\n",
       "      <td>74.40</td>\n",
       "      <td>85.0</td>\n",
       "      <td>336.0</td>\n",
       "      <td>10.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7905 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n",
       "id                                                                            \n",
       "0        999  D-penicillamine  21532   M       N            N       N     N   \n",
       "1       2574          Placebo  19237   F       N            N       N     N   \n",
       "2       3428          Placebo  13727   F       N            Y       Y     Y   \n",
       "3       2576          Placebo  18460   F       N            N       N     N   \n",
       "4        788          Placebo  16658   F       N            Y       N     N   \n",
       "...      ...              ...    ...  ..     ...          ...     ...   ...   \n",
       "7900    1166  D-penicillamine  16839   F       N            N       N     N   \n",
       "7901    1492          Placebo  17031   F       N            Y       N     N   \n",
       "7902    1576  D-penicillamine  25873   F       N            N       Y     S   \n",
       "7903    3584  D-penicillamine  22960   M       N            Y       N     N   \n",
       "7904    1978  D-penicillamine  19237   F       N            N       N     N   \n",
       "\n",
       "      Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  \\\n",
       "id                                                                \n",
       "0           2.3        316.0     3.35   172.0    1601.0  179.80   \n",
       "1           0.9        364.0     3.54    63.0    1440.0  134.85   \n",
       "2           3.3        299.0     3.55   131.0    1029.0  119.35   \n",
       "3           0.6        256.0     3.50    58.0    1653.0   71.30   \n",
       "4           1.1        346.0     3.65    63.0    1181.0  125.55   \n",
       "...         ...          ...      ...     ...       ...     ...   \n",
       "7900        0.8        309.0     3.56    38.0    1629.0   79.05   \n",
       "7901        0.9        260.0     3.43    62.0    1440.0  142.00   \n",
       "7902        2.0        225.0     3.19    51.0     933.0   69.75   \n",
       "7903        0.7        248.0     2.75    32.0    1003.0   57.35   \n",
       "7904        0.7        256.0     3.23    22.0     645.0   74.40   \n",
       "\n",
       "      Tryglicerides  Platelets  Prothrombin  Stage Status  \n",
       "id                                                         \n",
       "0              63.0      394.0          9.7    3.0      D  \n",
       "1              88.0      361.0         11.0    3.0      C  \n",
       "2              50.0      199.0         11.7    4.0      D  \n",
       "3              96.0      269.0         10.7    3.0      C  \n",
       "4              96.0      298.0         10.6    4.0      C  \n",
       "...             ...        ...          ...    ...    ...  \n",
       "7900          224.0      344.0          9.9    2.0      C  \n",
       "7901           78.0      277.0         10.0    4.0      C  \n",
       "7902           62.0      200.0         12.7    2.0      D  \n",
       "7903          118.0      221.0         10.6    4.0      D  \n",
       "7904           85.0      336.0         10.3    3.0      C  \n",
       "\n",
       "[7905 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RANDOM_SEED = 64\n",
    "\n",
    "input_folder = \"./data\" # /kaggle/input/playground-series-s3e26\n",
    "train_df = pd.read_csv(input_folder+ \"/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(input_folder+\"/test.csv\", index_col=\"id\")\n",
    "\n",
    "target_column = \"Status\"\n",
    "\n",
    "categorical_features = [\"Drug\", \"Sex\", \"Ascites\", \"Hepatomegaly\", \"Spiders\", \"Edema\", \"Stage\"]\n",
    "numerical_features = [\"N_Days\", \"Age\", \"Bilirubin\", \"Cholesterol\", \"Albumin\", \"Copper\", \"Alk_Phos\", \"SGOT\", \"Tryglicerides\", \"Platelets\", \"Prothrombin\"]\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    #train_df['Status'] = train_df['Status'].map({\"D\": 0,\"C\": 1,\"CL\": 2})\n",
    "    df['date_of_diagnosis'] = df['Age'] - df['N_Days']\n",
    "    df['diseases'] = df['Ascites'] + df['Hepatomegaly'] + df['Spiders'] + df['Edema']\n",
    "\n",
    "    # change categorical features to category type\n",
    "    for col in categorical_features:\n",
    "        df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    # change \"Stage\" to string\n",
    "    df[\"Stage\"] = df[\"Stage\"].apply(lambda x: str(x))\n",
    "    return df\n",
    "\n",
    "train_df = feature_engineering(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 7114\n",
      "Number of validation examples: 791\n",
      "Number of examples per class in training set\n",
      "Status\n",
      "C     4468\n",
      "D     2398\n",
      "CL     248\n",
      "Name: count, dtype: int64\n",
      "Number of examples per class in validation set\n",
      "Status\n",
      "C     497\n",
      "D     267\n",
      "CL     27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = train_df.drop(columns=target_column)\n",
    "y = train_df[target_column]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=RANDOM_SEED, stratify=y, shuffle=True)\n",
    "\n",
    "print(f\"Number of training examples: {len(X_train)}\")\n",
    "print(f\"Number of validation examples: {len(X_val)}\")\n",
    "\n",
    "print(\"Number of examples per class in training set\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"Number of examples per class in validation set\")\n",
    "print(y_val.value_counts())\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_val = le.transform(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_keys = np.unique(y_train)\n",
    "class_weight_values = compute_class_weight(\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "class_weights = dict(zip(class_weight_keys, class_weight_values))\n",
    "class_weights\n",
    "\n",
    "sample_weights = [ class_weights[j] for j in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgboost\": xgb.XGBClassifier(objective=\"multi:softprob\", random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"logistic_regression\": LogisticRegression(random_state=RANDOM_SEED, multi_class=\"ovr\", n_jobs=-1),\n",
    "    \"decision_tree\": DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    \"random_forest\": RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    \"catboost\": cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False),\n",
    "    \"lightgbm\": lgb.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, force_row_wise=True),\n",
    "    \"hist_gradient_boosting\": HistGradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    \"extra_trees\": ExtraTreesClassifier(random_state=RANDOM_SEED),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:44<00:31, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1956\n",
      "[LightGBM] [Info] Number of data points in the train set: 5691, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465201\n",
      "[LightGBM] [Info] Start training from score -3.353336\n",
      "[LightGBM] [Info] Start training from score -1.087603\n",
      "[LightGBM] [Info] Total Bins 1948\n",
      "[LightGBM] [Info] Number of data points in the train set: 5691, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.464921\n",
      "[LightGBM] [Info] Start training from score -3.358374\n",
      "[LightGBM] [Info] Start training from score -1.087603\n",
      "[LightGBM] [Info] Total Bins 1945\n",
      "[LightGBM] [Info] Number of data points in the train set: 5691, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.464921\n",
      "[LightGBM] [Info] Start training from score -3.358374\n",
      "[LightGBM] [Info] Start training from score -1.087603\n",
      "[LightGBM] [Info] Total Bins 1943\n",
      "[LightGBM] [Info] Number of data points in the train set: 5691, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465201\n",
      "[LightGBM] [Info] Start training from score -3.358374\n",
      "[LightGBM] [Info] Start training from score -1.087082\n",
      "[LightGBM] [Info] Total Bins 1944\n",
      "[LightGBM] [Info] Number of data points in the train set: 5692, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465376\n",
      "[LightGBM] [Info] Start training from score -3.353512\n",
      "[LightGBM] [Info] Start training from score -1.087257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:55<00:00,  6.22s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>log_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gradient_boosting</td>\n",
       "      <td>0.438751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hist_gradient_boosting</td>\n",
       "      <td>0.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>catboost</td>\n",
       "      <td>0.450839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lightgbm</td>\n",
       "      <td>0.451550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>0.453523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.458304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extra_trees</td>\n",
       "      <td>0.475076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic_regression</td>\n",
       "      <td>0.497911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>0.601768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model_name  log_loss\n",
       "4       gradient_boosting  0.438751\n",
       "7  hist_gradient_boosting  0.443300\n",
       "5                catboost  0.450839\n",
       "6                lightgbm  0.451550\n",
       "0                 xgboost  0.453523\n",
       "3           random_forest  0.458304\n",
       "8             extra_trees  0.475076\n",
       "1     logistic_regression  0.497911\n",
       "2           decision_tree  0.601768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dist = ['Bilirubin',   'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides',  'Prothrombin']\n",
    "normal_dist = ['N_Days', 'Age', 'Cholesterol', 'Albumin', 'Platelets',]\n",
    "\n",
    "numeric_transformer = Pipeline(\n",
    "    [\n",
    "        (\"power_transformer\", PowerTransformer()),\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    [\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "models_scores = []\n",
    "\n",
    "for model_name, model in tqdm(models.items()):\n",
    "    try:\n",
    "        clf = Pipeline(\n",
    "            [\n",
    "                (\"preprocessor\", preprocessor),\n",
    "                (\"classifier\", CalibratedClassifierCV(model, cv=5)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_proba = clf.predict_proba(X_val)\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        log_loss_score = log_loss(y_val, y_pred_proba)\n",
    "    except Exception as e:\n",
    "        print(\"Problem with model: \", model_name)\n",
    "        print(e)\n",
    "        log_loss_score = None\n",
    "\n",
    "    models_scores.append({\n",
    "        \"model_name\": model_name,\n",
    "        \"log_loss\": log_loss_score\n",
    "    })\n",
    "\n",
    "models_scores_df = pd.DataFrame(models_scores).sort_values(by=\"log_loss\", ascending=True)\n",
    "models_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Total Bins 1959\n",
      "[LightGBM] [Info] Number of data points in the train set: 7114, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465124\n",
      "[LightGBM] [Info] Start training from score -3.356391\n",
      "[LightGBM] [Info] Start training from score -1.087430\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m base_model \u001b[39m=\u001b[39m StackingClassifier(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     estimators\u001b[39m=\u001b[39m[ (k, v) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m models\u001b[39m.\u001b[39mitems() ],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m stacking_classifier_pipeline \u001b[39m=\u001b[39m Pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     [\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m\"\u001b[39m, preprocessor),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         (\u001b[39m\"\u001b[39m\u001b[39mclassifier\u001b[39m\u001b[39m\"\u001b[39m, base_model),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m stacking_classifier_pipeline\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m y_pred_proba \u001b[39m=\u001b[39m stacking_classifier_pipeline\u001b[39m.\u001b[39mpredict_proba(X_val)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/wiktor/code/kaggle-challenges/playground-series-s3e26/initial_comparison_and_ensemble.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m y_pred \u001b[39m=\u001b[39m stacking_classifier_pipeline\u001b[39m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/pipeline.py:420\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    419\u001b[0m         fit_params_last_step \u001b[39m=\u001b[39m fit_params_steps[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m]]\n\u001b[0;32m--> 420\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_final_estimator\u001b[39m.\u001b[39;49mfit(Xt, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_last_step)\n\u001b[1;32m    422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:658\u001b[0m, in \u001b[0;36mStackingClassifier.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mclasses_\n\u001b[1;32m    657\u001b[0m     y_encoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_label_encoder\u001b[39m.\u001b[39mtransform(y)\n\u001b[0;32m--> 658\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(X, y_encoded, sample_weight)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1149\u001b[0m     )\n\u001b[1;32m   1150\u001b[0m ):\n\u001b[0;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/ensemble/_stacking.py:206\u001b[0m, in \u001b[0;36m_BaseStacking.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mappend(estimator)\n\u001b[1;32m    202\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    203\u001b[0m     \u001b[39m# Fit the base estimators on the whole training data. Those\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     \u001b[39m# base estimators will be used in transform, predict, and\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[39m# predict_proba. They are exposed publicly.\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs)(\n\u001b[1;32m    207\u001b[0m         delayed(_fit_single_estimator)(clone(est), X, y, sample_weight)\n\u001b[1;32m    208\u001b[0m         \u001b[39mfor\u001b[39;49;00m est \u001b[39min\u001b[39;49;00m all_estimators\n\u001b[1;32m    209\u001b[0m         \u001b[39mif\u001b[39;49;00m est \u001b[39m!=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mdrop\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    210\u001b[0m     )\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnamed_estimators_ \u001b[39m=\u001b[39m Bunch()\n\u001b[1;32m    213\u001b[0m est_fitted_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_model = StackingClassifier(\n",
    "    estimators=[ (k, v) for k, v in models.items() ],\n",
    "    n_jobs=-1\n",
    ")\n",
    "stacking_classifier_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", base_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stacking_classifier_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba = stacking_classifier_pipeline.predict_proba(X_val)\n",
    "y_pred = stacking_classifier_pipeline.predict(X_val)\n",
    "\n",
    "log_loss_score = log_loss(y_val, y_pred_proba)\n",
    "\n",
    "models_scores.append({\n",
    "    \"model_name\": \"stacking_classifier\",\n",
    "    \"log_loss\": log_loss_score\n",
    "})\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "base_model = VotingClassifier(\n",
    "    estimators=[ (k, v) for k, v in models.items() ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "voting_classifier_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", base_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "voting_classifier_pipeline.fit(X_train, y_train)\n",
    "y_pred_proba = voting_classifier_pipeline.predict_proba(X_val)\n",
    "y_pred = voting_classifier_pipeline.predict(X_val)\n",
    "\n",
    "log_loss_score = log_loss(y_val, y_pred_proba)\n",
    "\n",
    "models_scores.append({\n",
    "    \"model_name\": \"voting_classifier\",\n",
    "    \"log_loss\": log_loss_score\n",
    "})\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "best_model_name = models_scores_df.iloc[1][\"model_name\"]\n",
    "base_model = BaggingClassifier(\n",
    "    estimator=clone_model(models[\"gradient_boosting\"]),\n",
    "    n_jobs=-1\n",
    ")\n",
    "clf = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", base_model),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba = clf.predict_proba(X_val)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "log_loss_score = log_loss(y_val, y_pred_proba)\n",
    "\n",
    "models_scores.append({\n",
    "    \"model_name\": \"bagging_classifier\",\n",
    "    \"log_loss\": log_loss_score\n",
    "})\n",
    "\n",
    "# -------------- RESULTS ----------------------\n",
    "\n",
    "models_scores_df = pd.DataFrame(models_scores).sort_values(by=\"log_loss\", ascending=True)\n",
    "# remove duplicates\n",
    "models_scores_df = models_scores_df.drop_duplicates(keep=\"first\")\n",
    "models_scores_df.to_csv(\"models_scores.csv\", index=False, header=True)\n",
    "models_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss for bagging_classifier: 0.4349009883095864\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.83      0.94      0.88       497\n",
      "          CL       0.50      0.11      0.18        27\n",
      "           D       0.83      0.70      0.76       267\n",
      "\n",
      "    accuracy                           0.83       791\n",
      "   macro avg       0.72      0.58      0.61       791\n",
      "weighted avg       0.82      0.83      0.82       791\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_model_name = models_scores_df.iloc[0][\"model_name\"]\n",
    "\n",
    "if best_model_name == \"voting_classifier\":\n",
    "    clf = clone_model(voting_classifier_pipeline)\n",
    "elif best_model_name == \"stacking_classifier\":\n",
    "    clf = clone_model(stacking_classifier_pipeline)\n",
    "else:\n",
    "    best_model = clone_model(models[\"gradient_boosting\"])\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", best_model),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_proba = clf.predict_proba(X_val)\n",
    "y_pred = clf.predict(X_val)\n",
    "\n",
    "print(f\"Log loss for {best_model_name}: {log_loss(y_val, y_pred_proba)}\")\n",
    "print(classification_report(y_val, y_pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5gAAAK9CAYAAABB3sOMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABTK0lEQVR4nO3debhd49k/8O/JdDLJKIMgEaIhBDWU1BBqiCGGmuekRVtCjaXxtqYiSo39KdoipZSiaFEEMYcqNZOKIkoGQwZCToazf3/0dd6exiY7Vuyc9PN5r31dOc9ae617H91v3L7Pep6aUqlUCgAAAHxBzapdAAAAAEsHDSYAAACF0GACAABQCA0mAAAAhdBgAgAAUAgNJgAAAIXQYAIAAFAIDSYAAACF0GACAABQCA0mAEmSV155Jdtss006duyYmpqa3HLLLYVe//XXX09NTU1Gjx5d6HWXBiuttFKGDx9e7TIA4AvTYAIsQV599dV897vfzcorr5zWrVunQ4cO2XjjjXPhhRfm448/Xqz3HjZsWJ577rmcccYZufrqq7P++usv1vstjV588cWccsopef3116tdCgBURU2pVCpVuwgAkttvvz177LFHamtrc+CBB2bNNdfMnDlz8vDDD+emm27K8OHD88tf/nKx3Pvjjz9O27Zt8z//8z85/fTTF8s9SqVS6urq0rJlyzRv3nyx3KPabrzxxuyxxx4ZO3ZsNt9884V+X11dXZo1a5aWLVsuvuIA4EvQotoFAJC89tpr2XvvvdOnT5/cd999WW655RqOjRgxIhMmTMjtt9++2O7/zjvvJEk6deq02O5RU1OT1q1bL7brNzWlUimzZ89OmzZtUltbW+1yAKAQpsgCLAHOPvvsfPjhh7n88ssbNZef6NevX4488siGn+fNm5ef/OQnWWWVVVJbW5uVVlopJ554Yurq6hq9b6WVVsrQoUPz8MMP52tf+1pat26dlVdeOVdddVXDOaecckr69OmTJPnBD36QmpqarLTSSkmS4cOHN/z5351yyimpqalpNDZmzJhssskm6dSpU9q3b5/+/fvnxBNPbDhe7hnM++67L5tuumnatWuXTp06Zeedd85LL730qfebMGFChg8fnk6dOqVjx4751re+lY8++qj8L/Z/bb755llzzTXz7LPPZvDgwWnbtm369euXG2+8MUnywAMPZMMNN0ybNm3Sv3//3HPPPY3e/8Ybb+Swww5L//7906ZNm3Tt2jV77LFHo6mwo0ePzh577JEk2WKLLVJTU5Oamprcf//9Sf7vn8Vdd92V9ddfP23atMlll13WcOyTZzBLpVK22GKLdOvWLVOnTm24/pw5czJw4MCsssoqmTVr1ud+ZgCoBg0mwBLgT3/6U1ZeeeV8/etfX6jzDz744Jx00klZd911c/7552fw4MEZNWpU9t577wXOnTBhQnbfffdsvfXWOffcc9O5c+cMHz48L7zwQpJk1113zfnnn58k2WeffXL11VfnggsuqKj+F154IUOHDk1dXV1OO+20nHvuudlpp53yyCOPfOb77rnnngwZMiRTp07NKaeckmOOOSaPPvpoNt544099jnHPPffMBx98kFGjRmXPPffM6NGjc+qppy5UjdOmTcvQoUOz4YYb5uyzz05tbW323nvvXH/99dl7772z/fbb56yzzsqsWbOy++6754MPPmh47xNPPJFHH300e++9dy666KJ873vfy7333pvNN9+8ocHdbLPN8v3vfz9JcuKJJ+bqq6/O1VdfndVXX73hOuPHj88+++yTrbfeOhdeeGHWWWedBeqsqanJFVdckdmzZ+d73/tew/jJJ5+cF154IVdeeWXatWu3UJ8ZAL50JQCqasaMGaUkpZ133nmhzn/66adLSUoHH3xwo/HjjjuulKR03333NYz16dOnlKT04IMPNoxNnTq1VFtbWzr22GMbxl577bVSktI555zT6JrDhg0r9enTZ4EaTj755NK//xVy/vnnl5KU3nnnnbJ1f3KPK6+8smFsnXXWKXXv3r303nvvNYw988wzpWbNmpUOPPDABe737W9/u9E1v/nNb5a6du1a9p6fGDx4cClJ6dprr20Ye/nll0tJSs2aNSs99thjDeN33XXXAnV+9NFHC1xz3LhxpSSlq666qmHshhtuKCUpjR07doHzP/lnceedd37qsWHDhjUau+yyy0pJSr/97W9Ljz32WKl58+alo4466nM/KwBUkwQToMpmzpyZJFlmmWUW6vw77rgjSXLMMcc0Gj/22GOTZIFnNQcMGJBNN9204edu3bqlf//++cc//rHINf+nT57dvPXWW1NfX79Q75k0aVKefvrpDB8+PF26dGkYX2uttbL11ls3fM5/9++JXpJsuummee+99xp+h5+lffv2jRLe/v37p1OnTll99dWz4YYbNox/8ud///20adOm4c9z587Ne++9l379+qVTp0556qmnFuLT/kvfvn0zZMiQhTr3O9/5ToYMGZIjjjgiBxxwQFZZZZWceeaZC30vAKgGDSZAlXXo0CFJGk3J/CxvvPFGmjVrln79+jUa79mzZzp16pQ33nij0Xjv3r0XuEbnzp0zbdq0Rax4QXvttVc23njjHHzwwenRo0f23nvv/P73v//MZvOTOvv377/AsdVXXz3vvvvuAs8a/udn6dy5c5Is1GdZYYUVFnhutGPHjllxxRUXGPvPa3788cc56aSTsuKKK6a2tjbLLrtsunXrlunTp2fGjBmfe+9P9O3bd6HPTZLLL788H330UV555ZWMHj26UaMLAEsiDSZAlXXo0CG9evXK888/X9H7/rNZKqfcliClhdilqtw95s+f3+jnNm3a5MEHH8w999yTAw44IM8++2z22muvbL311guc+0V8kc9S7r0Lc80jjjgiZ5xxRvbcc8/8/ve/z913350xY8aka9euC53YJqm4Qbz//vsbFm567rnnKnovAFSDBhNgCTB06NC8+uqrGTdu3Oee26dPn9TX1+eVV15pND5lypRMnz69YUXYInTu3DnTp09fYPw/U9IkadasWbbccsucd955efHFF3PGGWfkvvvuy9ixYz/12p/UOX78+AWOvfzyy1l22WWXmMVsbrzxxgwbNiznnntuw4JJm2yyyQK/m4Vt+hfGpEmTcsQRR2SbbbbJ0KFDc9xxx33q7x0AliQaTIAlwPHHH5927drl4IMPzpQpUxY4/uqrr+bCCy9Mkmy//fZJssBKr+edd16SZIcddiisrlVWWSUzZszIs88+2zA2adKk3HzzzY3Oe//99xd47ycrpP7n1imfWG655bLOOuvkN7/5TaNG7fnnn8/dd9/d8DmXBM2bN18gJf35z3++QDr7SUP8aU15pQ455JDU19fn8ssvzy9/+cu0aNEiBx100EKltQBQLS2qXQAA/2rkrr322uy1115ZffXVc+CBB2bNNdfMnDlz8uijj+aGG25o2Cdx7bXXzrBhw/LLX/4y06dPz+DBg/OXv/wlv/nNb7LLLrtkiy22KKyuvffeOyeccEK++c1v5vvf/34++uijXHLJJfnKV77SaHGb0047LQ8++GB22GGH9OnTJ1OnTs0vfvGLrLDCCtlkk03KXv+cc87Jdtttl0GDBuWggw7Kxx9/nJ///Ofp2LFjTjnllMI+xxc1dOjQXH311enYsWMGDBiQcePG5Z577knXrl0bnbfOOuukefPm+elPf5oZM2aktrY23/jGN9K9e/eK7nfllVfm9ttvz+jRo7PCCisk+VdDu//+++eSSy7JYYcdVthnA4AiaTABlhA77bRTnn322Zxzzjm59dZbc8kll6S2tjZrrbVWzj333BxyyCEN5/7617/OyiuvnNGjR+fmm29Oz549M3LkyJx88smF1tS1a9fcfPPNOeaYY3L88cenb9++GTVqVF555ZVGDeZOO+2U119/PVdccUXefffdLLvsshk8eHBOPfXUhkVzPs1WW22VO++8MyeffHJOOumktGzZMoMHD85Pf/rTihfEWZwuvPDCNG/ePNdcc01mz56djTfeuGEPz3/Xs2fPXHrppRk1alQOOuigzJ8/P2PHjq2owfznP/+Zo48+OjvuuGOGDRvWML7ffvvlpptuyvHHH5/ttttuifr9AMAnakrm2gAAAFAAz2ACAABQCA0mAAAAhdBgAgAAUAgNJgAAAIXQYAIAAFAIDSYAAACF0GACAABQiBbVLmBxmPvuP6pdArCYtOm1abVLABaTZdt2qHYJwGIyefpL1S5hkVWzt2i57MpVu/eikmACAABQiKUywQQAAChE/fxqV9CkSDABAAAohAYTAACAQpgiCwAAUE6pvtoVNCkSTAAAAAohwQQAACinXoJZCQkmAAAAhZBgAgAAlFHyDGZFJJgAAAAUQoMJAABAIUyRBQAAKMciPxWRYAIAAFAICSYAAEA5FvmpiAQTAACAQmgwAQAAKIQpsgAAAOXUz692BU2KBBMAAIBCSDABAADKschPRSSYAAAAFEKCCQAAUE69BLMSEkwAAAAKocEEAACgEKbIAgAAlFGyyE9FJJgAAAAUQoIJAABQjkV+KiLBBAAAoBAaTAAAAAphiiwAAEA5FvmpiAQTAACAQkgwAQAAyqmfX+0KmhQJJgAAAIWQYAIAAJTjGcyKSDABAAAohAYTAACAQpgiCwAAUE69KbKVkGACAABQCAkmAABAORb5qYgEEwAAgEJoMAEAACiEKbIAAADlWOSnIhJMAAAACiHBBAAAKKNUml/tEpoUCSYAAACFkGACAACUY5uSikgwAQAAKIQGEwAAgEKYIgsAAFCObUoqIsEEAACgEBJMAACAcizyUxEJJgAAAIXQYAIAAFAIU2QBAADKqZ9f7QqaFAkmAAAAhZBgAgAAlGORn4pIMAEAACiEBBMAAKCceglmJSSYAAAAFEKDCQAAQCFMkQUAACjHIj8VkWACAABQCAkmAABAORb5qYgEEwAAgEJoMAEAACiEKbIAAADlmCJbEQkmAAAAhZBgAgAAlFEqza92CU2KBBMAAIBCaDABAAAohCmyAAAA5VjkpyISTAAAAAohwQQAACinJMGshAQTAACAQkgwAQAAyvEMZkUkmAAAABRCgwkAAEAhTJEFAAAoxyI/FZFgAgAAUAgJJgAAQDkW+amIBBMAAIBCaDABAAAohCmyAAAA5VjkpyISTAAAgKXIWWedlZqamhx11FENY7Nnz86IESPStWvXtG/fPrvttlumTJnS6H0TJ07MDjvskLZt26Z79+75wQ9+kHnz5lV0bwkmAABAOU1skZ8nnngil112WdZaa61G40cffXRuv/323HDDDenYsWMOP/zw7LrrrnnkkUeSJPPnz88OO+yQnj175tFHH82kSZNy4IEHpmXLljnzzDMX+v4STAAAgKXAhx9+mP322y+/+tWv0rlz54bxGTNm5PLLL895552Xb3zjG1lvvfVy5ZVX5tFHH81jjz2WJLn77rvz4osv5re//W3WWWedbLfddvnJT36Siy++OHPmzFnoGjSYAAAA5dTXV+1VV1eXmTNnNnrV1dWVLXXEiBHZYYcdstVWWzUaf/LJJzN37txG46uttlp69+6dcePGJUnGjRuXgQMHpkePHg3nDBkyJDNnzswLL7yw0L8uDSYAAMASaNSoUenYsWOj16hRoz713Ouuuy5PPfXUpx6fPHlyWrVqlU6dOjUa79GjRyZPntxwzr83l58c/+TYwvIMJgAAwBJo5MiROeaYYxqN1dbWLnDem2++mSOPPDJjxoxJ69atv6zyPpUGEwAAoJwqblNSW1v7qQ3lf3ryySczderUrLvuug1j8+fPz4MPPpj/9//+X+66667MmTMn06dPb5RiTpkyJT179kyS9OzZM3/5y18aXfeTVWY/OWdhmCILAADQhG255ZZ57rnn8vTTTze81l9//ey3334Nf27ZsmXuvffehveMHz8+EydOzKBBg5IkgwYNynPPPZepU6c2nDNmzJh06NAhAwYMWOhaJJgAAADlNIFtSpZZZpmsueaajcbatWuXrl27NowfdNBBOeaYY9KlS5d06NAhRxxxRAYNGpSNNtooSbLNNttkwIABOeCAA3L22Wdn8uTJ+dGPfpQRI0YsVIr6CQ0mAADAUu78889Ps2bNsttuu6Wuri5DhgzJL37xi4bjzZs3z2233ZZDDz00gwYNSrt27TJs2LCcdtppFd2nplQqlYouvtrmvvuPapcALCZtem1a7RKAxWTZth2qXQKwmEye/lK1S1hkH//xZ1W7d5udjqvavReVBBMAAKCcKi7y0xRZ5AcAAIBCSDABAADKaQKL/CxJJJgs8X599e+z5sbb5awLLm00/vTzL+XbR/wwG2y5SzbcetcMO+wHmV1X1+icBx79S/Y55Kist8XO+fq2e+T7P6zsIWXgy3fC8Ydn3KO3Z9p74/P2P5/JTTdenq98ZZVqlwVU6IijD8md9/0+E978a55/5eFcec3Ps0q/lRqd02elFXPFb3+eFyY8klcmPpFfXnlelu3WtToFA4XQYLJEe+6l8bnh1jvylX59G40//fxL+d4xP8rXv7ZufverC3Pdry/KPrvtmGY1NQ3njBn7cEaedk522X7r3PSbi3P1JT/L9ltv/iV/AqBSm226US655DfZeNMds+32+6Rli5b58+3Xpm3bNtUuDajAoI03yJW/vjY7bL139vzmQWnZomWuv/nyhu9y27Ztcv3Nv06pVMpuOw3Pjtvum5atWubq636Rmn/7+xyqrlRfvVcTZBVZllgfffRx9vj2EfnRsSNy2W9+l9X6rZwfHvW9JMm+hxyVQRusmyO+c+CnvnfevPkZsvuwHHbQAdltxyFfZtksZlaR/e+z7LJdMvnt57LFN3bNQw8/Xu1yWIysIrt069q1c1549dHssv0BeezRv2bwFl/PtTf+Mv1X2jAffjArSbJMh/YZ//rj2eubB+ehB8ZVuWKK1KRXkf3DmVW7d5tdT6zavRdV1RLM++67LwMGDMjMmTMXODZjxoysscYaeeihh6pQGUuK08+9OJsN2iCDNvhqo/H3pk3Psy+OT5fOHbPfd4/JZkP3yfARP8hTzzzfcM5Lf5+QKe+8l2bNarL78BHZfKd9871jf5xX/vH6l/wpgC+qY8d/NR3vT5te3UKAL2SZDsskSaZPm5EkaVXbKqVSKXPq5jScUze7LvX19dlw0LpVqRH44qrWYF5wwQU55JBD0qHDgv+1smPHjvnud7+b884773OvU1dXl5kzZzZ61f3Hc3g0PXfcc39e+vurOep731rg2D/fmpQk+cUV12T3nbbNZef9JKt/pV8OOnJk3njzrSTJm2//7zmXX5PvDtsnF599ajos0z7fOvyEzJj5wZf3QYAvpKamJuf97NQ88shf8sIL46tdDrCIampq8pNRI/P4uCfz8kuvJEmeeuKZfDTr4/zo1OPSpk3rtG3bJieffnxatGiR7j26Vbli+Df19dV7NUFVazCfeeaZbLvttmWPb7PNNnnyySc/9zqjRo1Kx44dG71+euGln/s+llyTpryTsy64LGedfHxqa1stcLz+f2d177Hz9vnmDttk9a/0ywlHfjcr9V4hf7jt7iRJqf5f53xn2F7ZeotNssZqq+b0E49OTU1y132ScWgqfn7RmVljjf7Zd//Dql0K8AWc9bOTstqAVfO9g45tGHvvvWk5ZPhR2WbbzfPqW0/m7xP/kg4dO+SZp19o+HscaHqqtk3JlClT0rJly7LHW7RokXfeeedzrzNy5Mgcc8wxjcaaffDWF66P6nlx/Ct5f9r07PntwxvG5s+vz5NPP5/f/eFP+dO1v0qSrNK3d6P3rdyndyZPmZok6da1y7/OWen/zmnVqlVW6LVcJv3vOcCS7cILTs8O22+VLbbcNW/978wFoOk58+wfZashg/PNHQ7IpLenNDr2wNhHs9FXh6RLl06ZN39+Zs74IM+OfzC3vv5mlaqFT9FEk8RqqVqDufzyy+f5559Pv379PvX4s88+m+WWW+5zr1NbW5va2tpGY3PnvFtIjVTHRuutk5uvvqTR2I/OOC99+6yYg/bfIysuv1y6L9s1r7/xz0bnvPHmP7PJRhskSQas1i+tWrXMaxPfyrprr5kkmTtvXt6aNCW9enb/cj4IsMguvOD07LLzttly6z3yun/RhCbrzLN/lO2GbpVdhw7LxDfKBwDvvz89SbLxZhtm2W5dc9ef7/uSKgSKVrUGc/vtt8+Pf/zjbLvttmndunWjYx9//HFOPvnkDB06tErVUU3t2rXNqiuv1GisTZvW6dRhmYbxb+27Wy6+/Lfpv2rfrLbqKrn1jnvy2hv/zHmn/0+SpH27dtlz5+3zi8uvTs/uy6ZXzx658tobkyTbbGEVUliS/fyiM7PP3rtk192+nQ8++DA9/vdZrBkzPsjs2bOrXB2wsM762Un55h47ZPi+h+fDD2elW/dlkyQfzPwgs2f/a72Mvff7Zv4+/h957933s/7X1slPzjoxv/zFb/LqhNerWDnwRVRtm5IpU6Zk3XXXTfPmzXP44Yenf//+SZKXX345F198cebPn5+nnnoqPXr0qPjatilZ+gw//PhG25Qkya+v/n1+94c/ZebMD/KVfivn2MO+3ZBWJv9KLC+49Mr86c77UldXl4EDVssPj/xu+q3cpxofgYLYpmTpN2/Op6cc3z7o6Fx19e+/5Gr4MtmmZOlSbluKIw8bmeuvvSVJ8j8nH5O99t0lnTp3zJsT385VV16Xyy7+zZdYJV+WJr1NyfWnVu3ebfY6uWr3XlRV3QfzjTfeyKGHHpq77rorn5RRU1OTIUOG5OKLL07fvn0X6boaTFh6aTBh6aXBhKWXBnPRNMUGs2pTZJOkT58+ueOOOzJt2rRMmDAhpVIpq666ajp37lzNsgAAAP7FIj8VqWqD+YnOnTtngw02qHYZAAAAfAFLRIMJAACwRJJgVqRZtQsAAABg6aDBBAAAoBCmyAIAAJRTMkW2EhJMAAAACiHBBAAAKMciPxWRYAIAAFAIDSYAAACFMEUWAACgnFKp2hU0KRJMAAAACiHBBAAAKMciPxWRYAIAAFAICSYAAEA5EsyKSDABAAAohAYTAACAQpgiCwAAUE7JFNlKSDABAAAohAQTAACgjFJ9qdolNCkSTAAAAAqhwQQAAKAQpsgCAACUYx/MikgwAQAAKIQEEwAAoBzblFREggkAAEAhJJgAAADl2KakIhJMAAAACqHBBAAAoBCmyAIAAJRjm5KKSDABAAAohAQTAACgHAlmRSSYAAAAFEKDCQAAQCFMkQUAACinZB/MSkgwAQAAKIQEEwAAoByL/FREggkAAEAhNJgAAAAUwhRZAACAcuot8lMJCSYAAACFkGACAACUU7LITyUkmAAAABRCggkAAFCOZzArIsEEAACgEBpMAAAACmGKLAAAQBmleov8VEKCCQAAQCEkmAAAAOVY5KciEkwAAAAKocEEAACgEKbIAgAAlFOyyE8lJJgAAAAUQoIJAABQjkV+KiLBBAAAoBASTAAAgHLqPYNZCQkmAAAAhdBgAgAAUAhTZAEAAMqxyE9FJJgAAAAUQoIJAABQTskiP5WQYAIAAFAIDSYAAACFMEUWAACgHIv8VESCCQAAQCEkmAAAAGWU6i3yUwkJJgAAAIWQYAIAAJTjGcyKSDABAAAohAYTAACAQpgiCwAAUI4pshWRYAIAAFAICSYAAEA5JduUVEKCCQAAQCE0mAAAABTCFFkAAIByLPJTEQkmAAAAhZBgAgAAlFGSYFZEggkAAEAhJJgAAADlSDArIsEEAACgEBpMAAAACmGKLAAAQDn19dWuoEmRYAIAAFAICSYAAEA5FvmpiAQTAACAQmgwAQAAKIQpsgAAAOWYIlsRCSYAAACFkGACAACUUSpJMCshwQQAAKAQEkwAAIByPINZEQkmAAAAhdBgAgAAUAhTZAEAAMoxRbYiEkwAAAAKIcEEAAAooyTBrMhS2WB27r1ltUsAFpOaahcALDbTZ8+qdgkAfEGmyAIAAFCIpTLBBAAAKIQpshWRYAIAAFAICSYAAEA59dUuoGmRYAIAAFAICSYAAEAZtimpjAQTAACAQmgwAQAAKIQpsgAAAOWYIlsRCSYAAACF0GACAACUU1/F10K65JJLstZaa6VDhw7p0KFDBg0alD//+c8Nx2fPnp0RI0aka9euad++fXbbbbdMmTKl0TUmTpyYHXbYIW3btk337t3zgx/8IPPmzVv4Iv6XBhMAAKAJW2GFFXLWWWflySefzF//+td84xvfyM4775wXXnghSXL00UfnT3/6U2644YY88MADefvtt7Prrrs2vH/+/PnZYYcdMmfOnDz66KP5zW9+k9GjR+ekk06quJaaUqm01E0qbt+2b7VLABaTunlzql0CsJg0b9a82iUAi8ns2ROrXcIim77XFlW7d6frxy7ye7t06ZJzzjknu+++e7p165Zrr702u+++e5Lk5Zdfzuqrr55x48Zlo402yp///OcMHTo0b7/9dnr06JEkufTSS3PCCSfknXfeSatWrRb6vhJMAACAMkr1paq96urqMnPmzEavurq6z6x3/vz5ue666zJr1qwMGjQoTz75ZObOnZutttqq4ZzVVlstvXv3zrhx45Ik48aNy8CBAxuayyQZMmRIZs6c2ZCCLiwNJgAAwBJo1KhR6dixY6PXqFGjPvXc5557Lu3bt09tbW2+973v5eabb86AAQMyefLktGrVKp06dWp0fo8ePTJ58uQkyeTJkxs1l58c/+RYJWxTAgAAUE4Fi+0UbeTIkTnmmGMajdXW1n7quf3798/TTz+dGTNm5MYbb8ywYcPywAMPfBllNqLBBAAAWALV1taWbSj/U6tWrdKvX78kyXrrrZcnnngiF154Yfbaa6/MmTMn06dPb5RiTpkyJT179kyS9OzZM3/5y18aXe+TVWY/OWdhmSILAACwlKmvr09dXV3WW2+9tGzZMvfee2/DsfHjx2fixIkZNGhQkmTQoEF57rnnMnXq1IZzxowZkw4dOmTAgAEV3VeCCQAAUEapfsnfdGPkyJHZbrvt0rt373zwwQe59tprc//99+euu+5Kx44dc9BBB+WYY45Jly5d0qFDhxxxxBEZNGhQNtpooyTJNttskwEDBuSAAw7I2WefncmTJ+dHP/pRRowYsdAJ6ic0mAAAAE3Y1KlTc+CBB2bSpEnp2LFj1lprrdx1113ZeuutkyTnn39+mjVrlt122y11dXUZMmRIfvGLXzS8v3nz5rntttty6KGHZtCgQWnXrl2GDRuW0047reJa7IMJNCn2wYSll30wYenVlPfBfH/nwVW7d5dbv/xFer4oz2ACAABQCFNkAQAAyihVcZuSpkiCCQAAQCE0mAAAABTCFFkAAIByTJGtiAQTAACAQkgwAQAAyrDIT2UkmAAAABRCgwkAAEAhTJEFAAAoxxTZikgwAQAAKIQEEwAAoAyL/FRGggkAAEAhJJgAAABlSDArI8EEAACgEBpMAAAACmGKLAAAQBmmyFZGggkAAEAhJJgAAADllGqqXUGTIsEEAACgEBpMAAAACmGKLAAAQBkW+amMBBMAAIBCSDABAADKKNVb5KcSEkwAAAAKIcEEAAAowzOYlZFgAgAAUAgNJgAAAIUwRRYAAKCMUskiP5WQYAIAAFAICSYAAEAZFvmpjAQTAACAQmgwAQAAKIQpsgAAAGWU6i3yUwkJJgAAAIWQYAIAAJRRKlW7gqZFggkAAEAhJJgAAABleAazMhJMAAAACqHBBAAAoBCmyAIAAJRhimxlJJgAAAAUQoIJAABQhm1KKvOFE8yZM2fmlltuyUsvvVREPQAAADRRFTeYe+65Z/7f//t/SZKPP/4466+/fvbcc8+stdZauemmmwovEAAAgKah4gbzwQcfzKabbpokufnmm1MqlTJ9+vRcdNFFOf300wsvEAAAoFpK9TVVezVFFTeYM2bMSJcuXZIkd955Z3bbbbe0bds2O+ywQ1555ZXCCwQAAKBpqHiRnxVXXDHjxo1Lly5dcuedd+a6665LkkybNi2tW7cuvEAAAIBqKZWaZpJYLRU3mEcddVT222+/tG/fPr17987mm2+e5F9TZwcOHFh0fQAAADQRFTeYhx12WL72ta/lzTffzNZbb51mzf41y3bllVf2DCYAALBUKdVXu4KmpaZUWrSdXebMmZPXXnstq6yySlq0WLK202zftm+1SwAWk7p5c6pdArCYNG/WvNolAIvJ7NkTq13CIpswYEjV7t3vxbuqdu9FVfEiPx999FEOOuigtG3bNmussUYmTvzX/1iOOOKInHXWWYUXCAAAQNNQcYM5cuTIPPPMM7n//vsbLeqz1VZb5frrry+0OAAAgGqqL9VU7dUUVTy39ZZbbsn111+fjTbaKDU1//eh11hjjbz66quFFgcAAEDTUXGD+c4776R79+4LjM+aNatRwwkAANDU2aakMhVPkV1//fVz++23N/z8SVP561//OoMGDSquMgAAAJqUihPMM888M9ttt11efPHFzJs3LxdeeGFefPHFPProo3nggQcWR40AAAA0ARUnmJtsskmefvrpzJs3LwMHDszdd9+d7t27Z9y4cVlvvfUWR40AAABVUaqvqdqrKVqkDSxXWWWV/OpXvyq6FgAAAJqwihvMT/a9LKd3796LXAwAAMCSpFSqdgVNS8UN5korrfSZq8XOnz//CxUEAABA01Rxg/m3v/2t0c9z587N3/72t5x33nk544wzCisMAACg2prqs5DVUnGDufbaay8wtv7666dXr14555xzsuuuuxZSGAAAAE1LxavIltO/f/888cQTRV0OAACAJqbiBHPmzJmNfi6VSpk0aVJOOeWUrLrqqoUVBgAAUG31JVNkK1Fxg9mpU6cFFvkplUpZccUVc9111xVWGAAAAE1LxQ3m2LFjG/3crFmzdOvWLf369UuLFou0rSYAAMASqSTBrEjFHeHgwYMXRx0AAAA0cQvVYP7xj39c6AvutNNOi1wMAAAATddCNZi77LLLQl2spqYm8+fP/yL1AAAALDFKpWpX0LQsVINZX1+/uOsAAACgibMqDwAAQBm2KanMIjWYs2bNygMPPJCJEydmzpw5jY59//vfL6QwAAAAmpaKG8y//e1v2X777fPRRx9l1qxZ6dKlS9599920bds23bt312ACAAD8l2pW6RuOPvro7Ljjjpk2bVratGmTxx57LG+88UbWW2+9/OxnPyussOnTp+faa68t7HoAAACVKpVqqvZqiipuMJ9++ukce+yxadasWZo3b566urqsuOKKOfvss3PiiScWVtgbb7yRAw44oLDr0bRtvPHX8vsbf51XXn0sH370WobuuHWj45dedk4+/Oi1Rq+bbx1dnWKBL+S73zkwTz05Ju+9+3Lee/flPPTgHzNkyBbVLgsoSPv27XLOOSfn739/NNOm/T1jx/4h6623VrXLAgpScYPZsmXLNGv2r7d17949EydOTJJ07Ngxb775ZrHVwf9q265Nnn/upRxz9Ellz7n77vuzct8NGl7fGma6NjRF/3xrUk78n1HZcKPtstGg7TP2/kfyh5uuyIABX6l2aUABLrnk7Gy55ab59rePynrrbZ17730od9xxbXr16lHt0uBTlUrVezVFFT+D+dWvfjVPPPFEVl111QwePDgnnXRS3n333Vx99dVZc801F0eNkDF3P5Axdz/wmefU1c3J1CnvfkkVAYvL7bePafTzSSf9NN/9zgHZ8Gvr5sUX/16lqoAitG5dm29+c7vsvvvBefjhvyRJTj/9/Gy//Vb5zncOyCmnFPe4FVAdFSeYZ555ZpZbbrkkyRlnnJHOnTvn0EMPzTvvvJNf/vKXhRcIC2vTTTfKa68/kaeevjcXXPiTdOnSqdolAV9Qs2bNsueeO6Vdu7Z57PEnq10O8AW1aNEiLVq0SF1dXaPx2bNn5+tf36BKVcFnqy/VVO3VFFWcYK6//voNf+7evXvuvPPORbrxRRdd9JnH33rrrUW6Lv+d7hnzQP74x7vyxutvpu/KvXPKKT/IH24ZnW9svmvq6+urXR5QoTXXXC0PPfjHtG5dmw8/nJXd9zg4L730SrXLAr6gDz+clXHj/pqRI7+fl1+ekClT3slee+2cDTdcN6+++nq1ywMKUHGDefrpp2e//fZL3759v9CNzz///M89p3fv3p97Tl1d3QL/FaxUKqWmpml2/CyaG2+8reHPL7wwPs8/93Kef/HBbLbZRrn//kerWBmwKMaPfzXrb7BNOnZYJrvutkOuuPyCbLnVbppMWAocdNDRueyyc/Laa09k3rx5+dvfns/vf39rvvrVgdUuDShAxQ3mDTfckJNPPjkbbrhh9t9//+y5555ZdtllK77xa6+9VvF7Ps2oUaNy6qmnNhpr2aJjWrXsXMj1aZpef/3NvPvOe1l5lT4aTGiC5s6d25BmPPW357L+euvkiMMPzmEjTqhuYcAX9o9/vJGtt94zbdu2SYcOy2Ty5Km5+uqL89prE6tdGnyqprpdSLVU/AzmM888k2effTabb755fvazn6VXr17ZYYcdcu211+ajjz5a6Ovcd999GTBgQGbOnLnAsRkzZmSNNdbIQw899LnXGTlyZGbMmNHo1bJFp0o+EkuhXsv3TJeunTN58jvVLgUoQLNmzVJb26raZQAF+uijjzN58tR06tQxW2+9WW67bcznvwlY4tWUSl9sAdxHHnkk1157bW644YbMnj37UxvGT7PTTjtliy22yNFHH/2pxy+66KKMHTs2N998c8U1tW/7xabvsuRp165tVl6lT5Jk3GN35ITjf5IHHxyXae/PyLRp0zPyxCNz6y1/zpQp72TllfvkJ2f8MO3bt8+GG2ybOXPmVLl6ilQ3zz/Ppd3pp/8wd945Nm+++VaWWaZ99t57l/zguBHZfod9c++9n/8fHmm6mjdrXu0S+BJstdVmqampySuv/COrrLJSzjzzxMyeXZctt9w98+bNq3Z5LCazZzfdhPrxXrtW7d4bvv2Hqt17UVU8RfY/tWvXLm3atEmrVq3ywQcfLPT7nnnmmfz0pz8te3ybbbbJz35mqWr+Zd11B+bPd13X8PNPz/5xkuS3V9+Yo478UdZcc7Xst9+u6dipQyZNmpr77n0oPzntPM0lNEHduy2bK6+4MMst1z0zZnyQ5557SXMJS5GOHTvkJz85Icsv3zPvvz8jt9xyR04++RzNJSwlFinBfO2113Lttdfm2muvzfjx4zN48ODsu+++2X333dOxY8eFukbr1q3z/PPPp1+/fp96fMKECRk4cGA+/vjjSsuTYMJSTIIJSy8JJiy9JJiL5r8iwdxoo43yxBNPZK211sq3vvWt7LPPPll++eUrvvHyyy//mQ3ms88+27DfJgAAQDV8oecJ/wtV3GBuueWWueKKKzJgwIAvdOPtt98+P/7xj7PtttumdevWjY59/PHHOfnkkzN06NAvdA8AAAC+PF94kZ9FNWXKlKy77rpp3rx5Dj/88PTv3z9J8vLLL+fiiy/O/Pnz89RTT6VHjx4VX9sUWVh6mSILSy9TZGHp1ZSnyD663G5Vu/fXJ91UtXsvqi+8yM+i6tGjRx599NEceuihGTlyZD7pc2tqajJkyJBcfPHFi9RcAgAAUB1VazCTpE+fPrnjjjsybdq0TJgwIaVSKauuumo6d+5czbIAAACSJKVSTbVLaFKq2mB+onPnztlggw2qXQYAAABfQLNqFwAAAMDSYZEazIceeij7779/Bg0alLfeeitJcvXVV+fhhx8utDgAAIBqqq/iqymquMG86aabMmTIkLRp0yZ/+9vfUldXlySZMWNGzjzzzMILBAAAoGmouME8/fTTc+mll+ZXv/pVWrZs2TC+8cYb56mnniq0OAAAgGoqpaZqr6ao4gZz/Pjx2WyzzRYY79ixY6ZPn15ETQAAADRBFTeYPXv2zIQJExYYf/jhh7PyyisXUhQAAABNT8XblBxyyCE58sgjc8UVV6SmpiZvv/12xo0bl+OOOy4//vGPF0eNAAAAVVFfqnYFTUvFDeYPf/jD1NfXZ8stt8xHH32UzTbbLLW1tTnuuONyxBFHLI4aAQAAaAJqSqXSIvXkc+bMyYQJE/Lhhx9mwIABad++fdG1LbL2bftWuwRgMambN6faJQCLSfNmzatdArCYzJ49sdolLLL7euxZtXt/Y8rvq3bvRVVxgvmJVq1aZcCAAUXWAgAAQBNWcYO5xRZbpKam/JK599133xcqCAAAYEnRVLcLqZaKG8x11lmn0c9z587N008/neeffz7Dhg0rqi4AAACamIobzPPPP/9Tx0855ZR8+OGHX7ggAAAAmqaK98EsZ//9988VV1xR1OUAAACqrr6Kr6aosAZz3Lhxad26dVGXAwAAoImpeIrsrrvu2ujnUqmUSZMm5a9//Wt+/OMfF1YYAABAtVnkpzIVN5gdO3Zs9HOzZs3Sv3//nHbaadlmm20KKwwAAICmpaIGc/78+fnWt76VgQMHpnPnzourJgAAAJqgip7BbN68ebbZZptMnz59MZUDAACw5LDIT2UqXuRnzTXXzD/+8Y/FUQsAAABNWMUN5umnn57jjjsut912WyZNmpSZM2c2egEAACwtJJiVWehnME877bQce+yx2X777ZMkO+20U2pq/m9FpVKplJqamsyfP7/4KgEAAFjiLXSDeeqpp+Z73/texo4duzjrAQAAWGLYpqQyC91glkqlJMngwYMXWzEAAAA0XRU9g/nvU2IBAADg31W0D+ZXvvKVz20y33///S9UEAAAwJKiXsZWkYoazFNPPTUdO3ZcXLUAAADQhFXUYO69997p3r374qoFAABgiVJvkZ+KLPQzmJ6/BAAA4LMsdIP5ySqyAAAALDlGjRqVDTbYIMsss0y6d++eXXbZJePHj290zuzZszNixIh07do17du3z2677ZYpU6Y0OmfixInZYYcd0rZt23Tv3j0/+MEPMm/evIpqWegGs76+3vRYAADgv0qpiq+F9cADD2TEiBF57LHHMmbMmMydOzfbbLNNZs2a1XDO0UcfnT/96U+54YYb8sADD+Ttt9/Orrvu2nB8/vz52WGHHTJnzpw8+uij+c1vfpPRo0fnpJNOquj3VVNaCqPJ9m37VrsEYDGpmzen2iUAi0nzZs2rXQKwmMyePbHaJSyyW3ruW7V77zL52kV63zvvvJPu3bvngQceyGabbZYZM2akW7duufbaa7P77rsnSV5++eWsvvrqGTduXDbaaKP8+c9/ztChQ/P222+nR48eSZJLL700J5xwQt555520atVqoe5d0T6YAAAA/03qq/iqq6vLzJkzG73q6uo+t+YZM2YkSbp06ZIkefLJJzN37txstdVWDeesttpq6d27d8aNG5ckGTduXAYOHNjQXCbJkCFDMnPmzLzwwgsL/fvSYAIAACyBRo0alY4dOzZ6jRo16jPfU19fn6OOOiobb7xx1lxzzSTJ5MmT06pVq3Tq1KnRuT169MjkyZMbzvn35vKT458cW1gVbVMCAADw36S+irtpjBw5Msccc0yjsdra2s98z4gRI/L888/n4YcfXpyllaXBBAAAWALV1tZ+bkP57w4//PDcdtttefDBB7PCCis0jPfs2TNz5szJ9OnTG6WYU6ZMSc+ePRvO+ctf/tLoep+sMvvJOQvDFFkAAIAmrFQq5fDDD8/NN9+c++67L337Nl70dL311kvLli1z7733NoyNHz8+EydOzKBBg5IkgwYNynPPPZepU6c2nDNmzJh06NAhAwYMWOhaJJgAAABlNIUtN0aMGJFrr702t956a5ZZZpmGZyY7duyYNm3apGPHjjnooINyzDHHpEuXLunQoUOOOOKIDBo0KBtttFGSZJtttsmAAQNywAEH5Oyzz87kyZPzox/9KCNGjKgoRdVgAgAANGGXXHJJkmTzzTdvNH7llVdm+PDhSZLzzz8/zZo1y2677Za6uroMGTIkv/jFLxrObd68eW677bYceuihGTRoUNq1a5dhw4bltNNOq6gW+2ACTYp9MGHpZR9MWHo15X0wr19uv6rde69J11Tt3ovKM5gAAAAUQoMJAABAITyDCQAAUEZ99bbBbJIkmAAAABRCggkAAFBGfUSYlZBgAgAAUAgJJgAAQBlL3Z6Oi5kEEwAAgEJoMAEAACiEKbIAAABl2KakMhJMAAAACiHBBAAAKKO+2gU0MRJMAAAACqHBBAAAoBCmyAIAAJRhH8zKSDABAAAohAQTAACgDNuUVEaCCQAAQCE0mAAAABTCFFkAAIAy7INZGQkmAAAAhZBgAgAAlCHBrIwEEwAAgEJIMAEAAMoo2aakIhJMAAAACqHBBAAAoBCmyAIAAJRhkZ/KSDABAAAohAQTAACgDAlmZSSYAAAAFEKDCQAAQCFMkQUAACijVO0CmhgJJgAAAIWQYAIAAJRRX1PtCpoWCSYAAACFkGACAACUYZuSykgwAQAAKIQGEwAAgEKYIgsAAFCGKbKVkWACAABQCAkmAABAGaVqF9DESDABAAAohAYTAACAQpgiCwAAUEZ9TbUraFokmAAAABRCggkAAFCGbUoqI8EEAACgEBJMAACAMmxTUhkJJgAAAIXQYAIAAFAIU2QBAADKqDdJtiJLZYO5QZd+1S4BWEwemvpitUsAFpNLu25W7RIA+IKWygYTAACgCLYpqYxnMAEAACiEBhMAAIBCmCILAABQhiV+KiPBBAAAoBASTAAAgDIs8lMZCSYAAACFkGACAACUUV9T7QqaFgkmAAAAhdBgAgAAUAhTZAEAAMqot1FJRSSYAAAAFEKCCQAAUIb8sjISTAAAAAqhwQQAAKAQpsgCAACUUV/tApoYCSYAAACFkGACAACUYZuSykgwAQAAKIQEEwAAoAz5ZWUkmAAAABRCgwkAAEAhTJEFAAAowzYllZFgAgAAUAgJJgAAQBm2KamMBBMAAIBCaDABAAAohCmyAAAAZZggWxkJJgAAAIWQYAIAAJRhm5LKSDABAAAohAQTAACgjJKnMCsiwQQAAKAQGkwAAAAKYYosAABAGRb5qYwEEwAAgEJIMAEAAMqot8hPRSSYAAAAFEKDCQAAQCFMkQUAACjDBNnKSDABAAAohAQTAACgDIv8VEaCCQAAQCE0mAAAABTCFFkAAIAy6qtdQBMjwQQAAKAQEkwAAIAyShb5qYgEEwAAgEJIMAEAAMrwDGZlJJgAAAAUQoMJAABAIUyRBQAAKMMiP5WRYAIAAFAICSYAAEAZFvmpjAQTAACAQmgwAQAAKIQpsgAAAGXUlyzyUwkJJgAAAIWQYAIAAJQhv6yMBBMAAIBCSDABAADKqJdhVkSCCQAAQCE0mAAAABTCFFkAAIAySqbIVkSCCQAAQCEkmAAAAGXUV7uAJkaCCQAAQCE0mAAAABTCFFkAAIAy7INZGQkmAAAAhdBgAgAAlFGq4v9V4sEHH8yOO+6YXr16paamJrfcckvjz1Eq5aSTTspyyy2XNm3aZKuttsorr7zS6Jz3338/++23Xzp06JBOnTrloIMOyocfflhRHRpMAACAJm7WrFlZe+21c/HFF3/q8bPPPjsXXXRRLr300jz++ONp165dhgwZktmzZzecs99+++WFF17ImDFjctttt+XBBx/Md77znYrq8AwmAABAGU1lm5Ltttsu22233aceK5VKueCCC/KjH/0oO++8c5LkqquuSo8ePXLLLbdk7733zksvvZQ777wzTzzxRNZff/0kyc9//vNsv/32+dnPfpZevXotVB0STAAAgCVQXV1dZs6c2ehVV1dX8XVee+21TJ48OVtttVXDWMeOHbPhhhtm3LhxSZJx48alU6dODc1lkmy11VZp1qxZHn/88YW+lwYTAABgCTRq1Kh07Nix0WvUqFEVX2fy5MlJkh49ejQa79GjR8OxyZMnp3v37o2Ot2jRIl26dGk4Z2GYIgsAAFBGqVS9bUpGjhyZY445ptFYbW1tlapZOBpMAACAJVBtbW0hDWXPnj2TJFOmTMlyyy3XMD5lypSss846DedMnTq10fvmzZuX999/v+H9C8MUWQAAgDLqU6raqyh9+/ZNz549c++99zaMzZw5M48//ngGDRqUJBk0aFCmT5+eJ598suGc++67L/X19dlwww0X+l4STAAAgCbuww8/zIQJExp+fu211/L000+nS5cu6d27d4466qicfvrpWXXVVdO3b9/8+Mc/Tq9evbLLLrskSVZfffVsu+22OeSQQ3LppZdm7ty5Ofzww7P33nsv9AqyiQYTAACgyfvrX/+aLbbYouHnT57dHDZsWEaPHp3jjz8+s2bNyne+851Mnz49m2yySe688860bt264T3XXHNNDj/88Gy55ZZp1qxZdtttt1x00UUV1VFTquZTq4vJFitsXe0SgMXkoakvVrsEYDH5dbctPv8koEka/tZvq13CItux99Cq3ftPE2+r2r0XlWcwAQAAKIQpsgAAAGWUClxs57+BBBMAAIBCSDABAADKKHK7kP8GEkwAAAAKocEEAACgEKbIAgAAlLEU7uq4WEkwAQAAKIQEEwAAoIz6ahfQxEgwAQAAKIQGEwAAgEKYIgsAAFBGyT6YFZFgAgAAUAgJJgAAQBn1EsyKSDABAAAohAQTAACgjFJJglkJDSZLvGbNmmXYMQdk6123TJfuXfLu5Pdy1w135+oLr2l03reOG5Yd9tku7Tu2z/NPvJDzT7wob732VpWqBhbVpptsmGOPPTTrfnVgevXqmV13/3b++Me7ql0WsBB6bNg/ax66Q7oO7Ju2PTvnvm+fn4l3PdlwvEXb2qx34l7pve36qe3UPh+++U5euuKujL/6vkbX6bZev6x7wh5Z9qurpDS/lPdfeCNj9vtp5s+e+2V/JKBCGkyWePsctld2PnDHnHXU2Xnt72+k/9pfyQnnHpdZH8zKH664JUmy92F7Zddv7ZKzjj47k96cnG8fNzxn/3ZUhn/joMyt85cRNCXt2rXNs8++mCtHX5ebbri82uUAFWjRtjbvvzgxr1z3YL5x+VELHN/g5P2y3MZr5KEjLsmHb76TXoMHZqMzh+ejydPz5pinkvyrudz6t8fnuf/3pzz+o6tSP78+XQb0TqleigRNgQaTJd4a6w/II3c/msfu+0uSZMo/p2TLnbfIauv0bzhn94O+masvuiaP3D0uSTLqqJ/mD3+7IZsM2Thj/3h/NcoGFtGdd43NnXeNrXYZwCJ4a+yzeWvss2WPd19/1Uy48aFMHvdSkuTv14zNV/b/Rpb96soNDebXTtk/L11xd567+E8N75v56qTFWzh8Bov8VKbqi/zU19fniiuuyNChQ7Pmmmtm4MCB2WmnnXLVVVeZ70yS5IW/vph1N/5qVui7fJJkldVXzpobrJm/jH0iSbJc757p2qNrnnzobw3vmfXBR3np6ZezxnoDqlIzALCgqX99Jb23Xjdte3ZOkvT8+urpuHLPvP3Ac0mS1l07pNu6/fLxuzOy/a0nZa+nL862N/5Pum/wlWqWDVSgqglmqVTKTjvtlDvuuCNrr712Bg4cmFKplJdeeinDhw/PH/7wh9xyyy2feY26urrU1dU1Gqsv1adZTdV7Zwpy7cXXpe0ybfObB65I/fz6NGveLJf/9Mrcc/O/ntfo0q1LkmTau9MavW/aO9PSpVvnL71eAODTPf7jq/L1sw/Knk/+PPVz56VUX8qjx1+eKY+PT5Is06dbkmSdY3fNX0/7Xd5/4Y2ssscmGXL9yNyy5Q/zwWtTqlk+/6VKEsyKVLXBHD16dB588MHce++92WKLLRodu++++7LLLrvkqquuyoEHHlj2GqNGjcqpp57aaKzPMn3Tt8Mqi6Vmvnyb7zg4W33zGzn98FF5/e+vp98a/TLilEPz3pT3cteNY6pdHgCwkFb/1jbptm6/3DP83Mz657vpseFq2eiMYfloyrRMeuiFpNm/AoK//3ZsJvz+wSTJ+y+8keU2XiOr7jU4T531+2qWDyyEqsZ8v/vd73LiiScu0FwmyTe+8Y388Ic/zDXXXPMp7/w/I0eOzIwZMxq9+izTd3GVTBV870eH5HcXX5+xf7w/r738esbcdE9u/NVN2ffwvZMk77/zfpKk87KN08rO3Trn/XemLXA9AODL17x1y6z7wz3zxKnX5J9j/pZpL72Zl0ePyWt/fDxrfneHJMnHU6YnSab/vfEq8DMmvJ12y3f9sksGFkFVG8xnn3022267bdnj2223XZ555pnPvEZtbW06dOjQ6GV67NKltk3r1NfXNxqrn1+fmv/9r5yTJk7Oe1Pey7qbfLXheNv2bbP6OqvlhSdf/FJrBQA+XbMWLdK8VYuU/uPv9FJ9fdKsJkny4ZvvZNak99NxleUandNh5Z6Z9dZ7X1qt8O/qS6WqvZqiqk6Rff/999OjR4+yx3v06JFp0yRQ/+3GjXks+39/30x9a2pe+/sbWXXNftnjO7vlz9f/3754N15+cw74/r5567W3MunNSfn2ccPz7pT38vBdj1SxcmBRtGvXNv36/d9MlL4r9c7aa6+R99+fljfffLuKlQGfp0Xb2nTo+3//bte+d7d0WaN36qbNyqy338vkR1/K+j/aJ/Nnz82H/3w3PQetllV22yRPnPZ/M9ZeuPT2rHPsbnn/xTfy/gsT02+PTdNxlV65/zsXVeMjARWqKVVxqdbmzZtn8uTJ6dat26cenzJlSnr16pX58+dXdN0tVti6iPJYQrRp1ybf/sHwbLLtxum8bKe8O/m93Hfr2Fx1wW8zb+68hvO+ddywDN13+7Tv0D7PPfF8Ljjxovzztbc+48o0RQ9NlUov7QZvNij33nPjAuO/uer3Oejgo6tQEV+WX3db8JEZmpaeg1bPtjf+zwLjE37/YB4++pdp061j1h25V3pttmZqO7XPrLfezfhrxubFX/650fkDR+yY1YZvlVad2mXaixPz19Ovy9Qn/v5lfQwWg+Fv/bbaJSyyTZffsmr3fuite6t270VV1QazWbNm2W677VJbW/upx+vq6nLnnXdqMIEGGkxYemkwYemlwVw0TbHBrOoU2WHDhn3uOZ+1giwAAMDiVG+bkopUtcG88sorq3l7AAAACmS5VQAAAApR1QQTAABgSWaKbGUkmAAAABRCggkAAFBGFTfdaJIkmAAAABRCgwkAAEAhTJEFAAAowyI/lZFgAgAAUAgJJgAAQBklCWZFJJgAAAAUQoMJAABAIUyRBQAAKMM+mJWRYAIAAFAICSYAAEAZtimpjAQTAACAQkgwAQAAyvAMZmUkmAAAABRCgwkAAEAhTJEFAAAowyI/lZFgAgAAUAgJJgAAQBklCWZFJJgAAAAUQoMJAABAIUyRBQAAKKPePpgVkWACAABQCAkmAABAGRb5qYwEEwAAgEJIMAEAAMrwDGZlJJgAAAAUQoMJAABAIUyRBQAAKMMiP5WRYAIAAFAICSYAAEAZFvmpjAQTAACAQmgwAQAAKIQpsgAAAGVY5KcyEkwAAAAKIcEEAAAowyI/lZFgAgAAUAgJJgAAQBmewayMBBMAAIBCaDABAAAohCmyAAAAZZRK9dUuoUmRYAIAAFAICSYAAEAZ9Rb5qYgEEwAAgEJoMAEAACiEKbIAAABllEqmyFZCggkAAEAhJJgAAABlWOSnMhJMAAAACiHBBAAAKMMzmJWRYAIAAFAIDSYAAACFMEUWAACgjHpTZCsiwQQAAKAQEkwAAIAySrYpqYgEEwAAgEJoMAEAACiEKbIAAABl2AezMhJMAAAACiHBBAAAKKPeIj8VkWACAABQCAkmAABAGZ7BrIwEEwAAgEJoMAEAACiEKbIAAABl1JsiWxEJJgAAAIWQYAIAAJRhkZ/KSDABAAAohAYTAACAQpgiCwAAUEZ9TJGthAQTAACAQkgwAQAAyrDIT2UkmAAAABRCggkAAFBGvQSzIhJMAAAACqHBBAAAoBCmyAIAAJRRsk1JRSSYAAAAFEKCCQAAUIZFfiojwQQAAKAQGkwAAAAKYYosAABAGSVTZCsiwQQAAKAQEkwAAIAybFNSGQkmAAAAhdBgAgAAUAhTZAEAAMqwyE9lJJgAAAAUQoIJAABQhgSzMhJMAAAACiHBBAAAKEN+WRkJJgAAAIXQYAIAAFCImpKnVmnC6urqMmrUqIwcOTK1tbXVLgcokO83LL18v2HppcGkSZs5c2Y6duyYGTNmpEOHDtUuByiQ7zcsvXy/YelliiwAAACF0GACAABQCA0mAAAAhdBg0qTV1tbm5JNPtkAALIV8v2Hp5fsNSy+L/AAAAFAICSYAAACF0GACAABQCA0mAAAAhdBgAgAAUAgNJk3W5MmTc8QRR2TllVdObW1tVlxxxey444659957q10aUKHP+z6vtNJKueCCC6pbJPCFDB8+PDU1NampqUnLli3To0ePbL311rniiitSX19f7fKAgrSodgGwKF5//fVsvPHG6dSpU84555wMHDgwc+fOzV133ZURI0bk5ZdfrnaJwELyfYb/Httuu22uvPLKzJ8/P1OmTMmdd96ZI488MjfeeGP++Mc/pkUL/2oKTZ1vMU3SYYcdlpqamvzlL39Ju3btGsbXWGONfPvb365iZUClfJ/hv0dtbW169uyZJFl++eWz7rrrZqONNsqWW26Z0aNH5+CDD65yhcAXZYosTc7777+fO++8MyNGjGj0L6Of6NSp05dfFLBIfJ+Bb3zjG1l77bXzhz/8odqlAAXQYNLkTJgwIaVSKauttlq1SwG+IN9nIElWW221vP7669UuAyiABpMmp1QqVbsEoCC+z0Dyr/9fUFNTU+0ygAJoMGlyVl111dTU1Fj4A5YCvs9Akrz00kvp27dvtcsACqDBpMnp0qVLhgwZkosvvjizZs1a4Pj06dO//KKAReL7DNx333157rnnsttuu1W7FKAAGkyapIsvvjjz58/P1772tdx000155ZVX8tJLL+Wiiy7KoEGDql0eUIGF/T6/9dZbefrppxu9pk2bVsXKgUrV1dVl8uTJeeutt/LUU0/lzDPPzM4775yhQ4fmwAMPrHZ5QAFqSh6AoYmaNGlSzjjjjNx2222ZNGlSunXrlvXWWy9HH310Nt9882qXB1Tg877PK620Ut54440F3nf11Vdn//33r0LFQKWGDx+e3/zmN0mSFi1apHPnzll77bWz7777ZtiwYWnWTO4BSwMNJgAAAIXwn4oAAAAohAYTAACAQmgwAQAAKIQGEwAAgEJoMAEAACiEBhMAAIBCaDABAAAohAYTAACAQmgwAVgow4cPzy677NLw8+abb56jjjrqS6/j/vvvT01NTaZPn77Y7vH666+npqYmTz/99GK7BwAsjTSYAE3Y8OHDU1NTk5qamrRq1Sr9+vXLaaedlnnz5i32e//hD3/IT37yk4U698toCgGA6mtR7QIA+GK23XbbXHnllamrq8sdd9yRESNGpGXLlhk5cuQC586ZMyetWrUq5L5dunQp5DoAwNJDggnQxNXW1qZnz57p06dPDj300Gy11Vb54x//mOT/prWeccYZ6dWrV/r3758kefPNN7PnnnumU6dO6dKlS3beeee8/vrrDdecP39+jjnmmHTq1Cldu3bN8ccfn1Kp1Oi+/zlFtq6uLieccEJWXHHF1NbWpl+/frn88svz+uuvZ4sttkiSdO7cOTU1NRk+fHiSpL6+PqNGjUrfvn3Tpk2brL322rnxxhsb3eeOO+7IV77ylbRp0yZbbLFFozo/zb777pu99tqr0djcuXOz7LLL5qqrrkqS3Hnnndlkk00aPt/QoUPz6quvlr3m6NGj06lTp0Zjt9xyS2pqahqN3XrrrVl33XXTunXrrLzyyjn11FMb0uRSqZRTTjklvXv3Tm1tbXr16pXvf//7n/lZAKCp0WACLGXatGmTOXPmNPx87733Zvz48RkzZkxuu+22zJ07N0OGDMkyyyyThx56KI888kjat2+fbbfdtuF95557bkaPHp0rrrgiDz/8cN5///3cfPPNn3nfAw88ML/73e9y0UUX5aWXXspll12W9u3bZ8UVV8xNN92UJBk/fnwmTZqUCy+8MEkyatSoXHXVVbn00kvzwgsv5Oijj87++++fBx54IMm/GuFdd901O+64Y55++ukcfPDB+eEPf/iZdey3337505/+lA8//LBh7K677spHH32Ub37zm0mSWbNm5Zhjjslf//rX3HvvvWnWrFm++c1vpr6+vsLf9v956KGHcuCBB+bII4/Miy++mMsuuyyjR4/OGWeckSS56aabcv755+eyyy7LK6+8kltuuSUDBw5c5PsBwBKpBECTNWzYsNLOO+9cKpVKpfr6+tKYMWNKtbW1peOOO67heI8ePUp1dXUN77n66qtL/fv3L9XX1zeM1dXVldq0aVO66667SqVSqbTccsuVzj777Ibjc+fOLa2wwgoN9yqVSqXBgweXjjzyyFKpVCqNHz++lKQ0ZsyYT61z7NixpSSladOmNYzNnj271LZt29Kjjz7a6NyDDjqotM8++5RKpVJp5MiRpQEDBjQ6fsIJJyxwrX83d+7c0rLLLlu66qqrGsb22Wef0l577fWp55dKpdI777xTSlJ67rnnSqVSqfTaa6+VkpT+9re/lUqlUunKK68sdezYsdF7br755tK//zW65ZZbls4888xG51x99dWl5ZZbrlQqlUrnnntu6Stf+Uppzpw5ZesAgKbOM5gATdxtt92W9u3bZ+7cuamvr8++++6bU045peH4wIEDGz13+cwzz2TChAlZZpllGl1n9uzZefXVVzNjxoxMmjQpG264YcOxFi1aZP31119gmuwnnn766TRv3jyDBw9e6LonTJiQjz76KFtvvXWj8Tlz5uSrX/1qkuSll15qVEeSDBo06DOv26JFi+y555655pprcsABB2TWrFm59dZbc9111zWc88orr+Skk07K448/nnfffbchuZw4cWLWXHPNhf4M/+6ZZ57JI4880pBYJv+aajx79ux89NFH2WOPPXLBBRdk5ZVXzrbbbpvtt98+O+64Y1q08FcxAEsPf6sBNHFbbLFFLrnkkrRq1Sq9evVaoGFp165do58//PDDrLfeernmmmsWuFa3bt0WqYY2bdpU/J5PprDefvvtWX755Rsdq62tXaQ6PrHffvtl8ODBmTp1asaMGZM2bdpk2223bTi+4447pk+fPvnVr36VXr16pb6+PmuuuWajqcX/rlmzZgs013Pnzl3g85x66qnZddddF3h/69ats+KKK2b8+PG55557MmbMmBx22GE555xz8sADD6Rly5Zf6PMCwJJCgwnQxLVr1y79+vVb6PPXXXfdXH/99enevXs6dOjwqecst9xyefzxx7PZZpslSebNm5cnn3wy66677qeeP3DgwNTX1+eBBx7IVltttcDxTxLU+fPnN4wNGDAgtbW1mThxYtnkc/XVV29YsOgTjz322Od+xq9//etZccUVc/311+fPf/5z9thjj4Ym7r333sv48ePzq1/9KptuummS5OGHH/7M63Xr1i0ffPBBZs2a1dCw/+cemeuuu27Gjx//mf8s2rRpkx133DE77rhjRowYkdVWWy3PPfdc2d8rADQ1GkyA/zL77bdfzjnnnOy888457bTTssIKK+SNN97IH/7whxx//PFZYYUVcuSRR+ass87KqquumtVWWy3nnXfeZ+5hudJKK2XYsGH59re/nYsuuihrr7123njjjUydOjV77rln+vTpk5qamtx2223Zfvvt06ZNmyyzzDI57rjjcvTRR6e+vj6bbLJJZsyYkUceeSQdOnTIsGHD8r3vfS/nnntufvCDH+Tggw/Ok08+mdGjRy/U59x3331z6aWX5u9//3vGjh3bMN65c+d07do1v/zlL7Pccstl4sSJn7tw0IYbbpi2bdvmxBNPzPe///08/vjjC9Rx0kknZejQoendu3d23333NGvWLM8880yef/75nH766Rk9enTmz5/fcK3f/va3adOmTfr06bNQnwcAmgKryAL8l2nbtm0efPDB9O7dO7vuumtWX331HHTQQZk9e3ZDonnsscfmgAMOyLBhwzJo0KAss8wyDSuwlnPJJZdk9913z2GHHZbVVlsthxxySGbNmpUkWX755XPqqafmhz/8YXr06JHDDz88SfKTn/wkP/7xjzNq1Kisvvrq2XbbbXP77benb9++SZLevXvnpptuyi233JK11147l156ac4888yF+pz77bdfXnzxxSy//PLZeOONG8abNWuW6667Lk8++WTWXHPNHH300TnnnHM+81pdunTJb3/729xxxx0ZOHBgfve73zV6zjVJhgwZkttuuy133313Nthgg2y00UY5//zzGxrITp065Ve/+lU23njjrLXWWrnnnnvypz/9KV27dl2ozwMATUFNqdyKDQAAAFABCSYAAACF0GACAABQCA0mAAAAhdBgAgAAUAgNJgAAAIXQYAIAAFAIDSYAAACF0GACAABQCA0mAAAAhdBgAgAAUAgNJgAAAIX4/22yusQ4jj2dAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualise confusion matrix\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.title.set_text(\"Confusion matrix\")\n",
    "sns.heatmap(confusion_matrix(y_val, y_pred), annot=True, fmt='g', ax=ax)\n",
    "ax.set_xticklabels(le.classes_)\n",
    "ax.set_yticklabels(le.classes_)\n",
    "\n",
    "# show on top of heatmap text \"Predicted values\"\n",
    "ax.set_xlabel(\"Predicted values\")\n",
    "ax.set_ylabel(\"True values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model1 ...\n",
      "[LightGBM] [Info] Total Bins 1963\n",
      "[LightGBM] [Info] Number of data points in the train set: 7905, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "Fitting model2 ...\n",
      "[LightGBM] [Info] Total Bins 1963\n",
      "[LightGBM] [Info] Number of data points in the train set: 7905, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "[LightGBM] [Info] Total Bins 1950\n",
      "[LightGBM] [Info] Total Bins 1954\n",
      "[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 28\n",
      "[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "[LightGBM] [Info] Total Bins 1957\n",
      "[LightGBM] [Info] Total Bins 1951\n",
      "[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 28[LightGBM] [Info] \n",
      "Total Bins 1955\n",
      "[LightGBM] [Info] Number of data points in the train set: 6324, number of used features: 28\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -0.465082\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -3.358480\n",
      "[LightGBM] [Info] Start training from score -1.087291\n",
      "[LightGBM] [Info] Start training from score -1.087291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wiktor/.pyenv/versions/3.10.11/envs/kaggling/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model3 ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Status_C</th>\n",
       "      <th>Status_CL</th>\n",
       "      <th>Status_D</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7905</th>\n",
       "      <td>0.537007</td>\n",
       "      <td>0.036047</td>\n",
       "      <td>0.426946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7906</th>\n",
       "      <td>0.756264</td>\n",
       "      <td>0.072493</td>\n",
       "      <td>0.171243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7907</th>\n",
       "      <td>0.057828</td>\n",
       "      <td>0.020666</td>\n",
       "      <td>0.921506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7908</th>\n",
       "      <td>0.943238</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>0.049443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7909</th>\n",
       "      <td>0.801923</td>\n",
       "      <td>0.035379</td>\n",
       "      <td>0.162698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13171</th>\n",
       "      <td>0.898827</td>\n",
       "      <td>0.031478</td>\n",
       "      <td>0.069695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13172</th>\n",
       "      <td>0.949209</td>\n",
       "      <td>0.008309</td>\n",
       "      <td>0.042482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13173</th>\n",
       "      <td>0.899351</td>\n",
       "      <td>0.011937</td>\n",
       "      <td>0.088712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13174</th>\n",
       "      <td>0.970943</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.022101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13175</th>\n",
       "      <td>0.349705</td>\n",
       "      <td>0.019028</td>\n",
       "      <td>0.631267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5271 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Status_C  Status_CL  Status_D\n",
       "id                                  \n",
       "7905   0.537007   0.036047  0.426946\n",
       "7906   0.756264   0.072493  0.171243\n",
       "7907   0.057828   0.020666  0.921506\n",
       "7908   0.943238   0.007319  0.049443\n",
       "7909   0.801923   0.035379  0.162698\n",
       "...         ...        ...       ...\n",
       "13171  0.898827   0.031478  0.069695\n",
       "13172  0.949209   0.008309  0.042482\n",
       "13173  0.899351   0.011937  0.088712\n",
       "13174  0.970943   0.006956  0.022101\n",
       "13175  0.349705   0.019028  0.631267\n",
       "\n",
       "[5271 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"./data/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(\"./data/test.csv\", index_col=\"id\")\n",
    "\n",
    "train_df = feature_engineering(train_df)\n",
    "test_df = feature_engineering(test_df)\n",
    "\n",
    "# change categorical features to category type\n",
    "for col in categorical_features:\n",
    "    train_df[col] = train_df[col].astype(\"category\")\n",
    "    test_df[col] = test_df[col].astype(\"category\")\n",
    "\n",
    "X_train = train_df.drop(columns=target_column)\n",
    "y_train = le.transform(train_df[target_column])\n",
    "\n",
    "X_test = test_df\n",
    "\n",
    "model_1 : Pipeline = clone_model(voting_classifier_pipeline)\n",
    "model_2 : Pipeline = clone_model(stacking_classifier_pipeline)\n",
    "model_3 = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", clone_model(models[\"gradient_boosting\"])),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "print(\"Fitting model1 ...\")\n",
    "model_1.fit(X_train, y_train)\n",
    "print(\"Fitting model2 ...\")\n",
    "model_2.fit(X_train, y_train)\n",
    "print(\"Fitting model3 ...\")\n",
    "model_3.fit(X_train, y_train)\n",
    "\n",
    "y_pred_proba_1 = model_1.predict_proba(X_test)\n",
    "y_pred_proba_2 = model_2.predict_proba(X_test)\n",
    "y_pred_proba_3 = model_3.predict_proba(X_test)\n",
    "\n",
    "measure_of_agreement = np.mean\n",
    "\n",
    "y_pred_proba = measure_of_agreement([y_pred_proba_1, y_pred_proba_2, y_pred_proba_3], axis=0)\n",
    "\n",
    "submission_df = pd.DataFrame(y_pred_proba, index=X_test.index, columns=[f\"Status_{target}\" for target in le.classes_])\n",
    "submission_df.to_csv(\"./submission.csv\")\n",
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
