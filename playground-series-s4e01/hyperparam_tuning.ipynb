{"cells":[{"cell_type":"markdown","metadata":{},"source":["# âš½S4E1 - EDA & initial submission - Binary Classification with a Bank Churn Dataset \n","\n","Welcome to 2024! For this Episode of the Series, your task is to predict whether a customer continues with their account or closes it (e.g., churns). Good luck!\n","\n","## Evaluation\n","\n","Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n","\n","## Submission Format\n","\n","For each id in the test set, you must predict the probability for the target variable Exited. The file should contain a header and have the following format:\n","\n","```\n","id,Exited\n","0,0.9\n","1,0.1\n","2,0.5\n","etc.\n","```\n","\n","## Data Description\n","\n","The dataset for this competition (both train and test) was generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. \n","\n","# Code\n","\n","## ToC\n","\n","- [Imports](#Imports)\n","\n","\n","## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:03:46.497400Z","iopub.status.busy":"2024-01-04T20:03:46.496914Z","iopub.status.idle":"2024-01-04T20:03:59.746581Z","shell.execute_reply":"2024-01-04T20:03:59.745260Z","shell.execute_reply.started":"2024-01-04T20:03:46.497360Z"},"trusted":true},"outputs":[],"source":["# essentials\n","import os\n","import pathlib\n","from copy import copy\n","import json\n","\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","\n","# visualisation\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# sklearn imports\n","import sklearn\n","from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MaxAbsScaler, PowerTransformer, FunctionTransformer, StandardScaler\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MaxAbsScaler, PowerTransformer, FunctionTransformer, StandardScaler\n","from sklearn.pipeline import Pipeline, make_pipeline, make_union, FeatureUnion\n","from sklearn.compose import ColumnTransformer\n","from sklearn.feature_selection import SelectKBest, chi2, f_classif, SequentialFeatureSelector, RFECV\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.base import clone as clone_model\n","from sklearn.metrics import classification_report, confusion_matrix, log_loss\n","from sklearn.impute import SimpleImputer, MissingIndicator, KNNImputer\n","\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression, ElasticNet, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, TweedieRegressor\n","from sklearn.svm import SVC, LinearSVC, NuSVC\n","from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import RocCurveDisplay, roc_auc_score, make_scorer, roc_curve\n","\n","from sklearn.preprocessing import Binarizer, Normalizer, RobustScaler, StandardScaler\n","from sklearn.preprocessing import FunctionTransformer\n","\n","# others\n","import xgboost as xgb \n","import lightgbm as lgb\n","import catboost as cb\n","\n","import optuna\n","import shap\n","\n","RANDOM_SEED = 64\n","\n","palette = [\"#4464ad\", \"#dc136c\", \"#F4FF52\", \"#f58f29\",\"#45cb85\"]\n","\n","sns.set_theme(style=\"whitegrid\")\n","sns.set_palette(palette)\n","sns.palplot(palette)"]},{"cell_type":"markdown","metadata":{},"source":["## Data loading & EDA\n","\n","First we will check\n","\n","1. Number and types of columns\n","2. Number of rows in train and test\n","2. Missing values\n","3. Target variable distribution"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:04:02.461936Z","iopub.status.busy":"2024-01-04T20:04:02.461081Z","iopub.status.idle":"2024-01-04T20:04:03.308882Z","shell.execute_reply":"2024-01-04T20:04:03.307180Z","shell.execute_reply.started":"2024-01-04T20:04:02.461876Z"},"trusted":true},"outputs":[],"source":["IN_KAGGLE = False\n","\n","kaggle_folder = \"/kaggle/input/\"\n","local_folder = \"./data/\"\n","input_folder = kaggle_folder if IN_KAGGLE else local_folder\n","\n","train_df = pd.read_csv(input_folder + \"playground-series-s4e1/train.csv\", index_col=\"id\")\n","test_df = pd.read_csv(input_folder + \"playground-series-s4e1/test.csv\", index_col=\"id\")\n","submission_df = pd.read_csv(input_folder + \"playground-series-s4e1/sample_submission.csv\")\n","original_df = pd.read_csv(input_folder + \"bank-customer-churn-prediction/Churn_Modelling.csv\")\n","target_col = \"Exited\"\n","\n","numeric_features = ['CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n","categorical_features = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n","\n","features_to_drop = ['CustomerId', 'Surname']\n","\n","GENERATED_COLUMN = True\n","ADD_ORIGINAL_DF = True\n","\n","model_postfix = \"_with_original\" if ADD_ORIGINAL_DF else \"\"\n","model_postfix += \"_generated\" if GENERATED_COLUMN else \"\"\n","\n","original_df = original_df.drop(columns=['RowNumber'])\n","\n","# drop na rows from orignal df\n","original_df = original_df.dropna()\n","\n","if GENERATED_COLUMN:\n","    train_df['generated'] = 1\n","    test_df['generated'] = 1\n","    original_df['generated'] = 0\n","    categorical_features.append('generated')\n","    \n","if ADD_ORIGINAL_DF:\n","    train_df = pd.concat([train_df, original_df])\n","\n","\n","for f in features_to_drop:\n","    if f in numeric_features:\n","        numeric_features.remove(f)\n","    if f in categorical_features:\n","        categorical_features.remove(f)\n","    \n","    train_df = train_df.drop(columns=f)\n","\n","def initial_feature_engineering(df):\n","    df['HasCrCard'] = df['HasCrCard'].astype('bool')\n","    df['IsActiveMember'] = df['IsActiveMember'].astype('bool')\n","    df['Gender'] = df['Gender'].map({ \"Male\": 0, \"Female\": 1}).astype(\"bool\")\n","    # encode geography\n","    df = pd.get_dummies(df, columns=['Geography'])\n","\n","    return df\n","\n","def feature_engineering_1(df):\n","    # Balance\n","    df['balance_over_100k'] = df['Balance'] >= 100000\n","    df['balance_over_150k'] = df['Balance'] >= 150000\n","\n","    # EstimatedSalary\n","    df[\"estimated_salary_under_50k\"] = df[\"EstimatedSalary\"] < 50000\n","    df[\"estimated_salary_50k_to_100k\"] = (df[\"EstimatedSalary\"] >= 50000) & (df[\"EstimatedSalary\"] < 100000)\n","    df[\"estamated_salary_over_150k\"] = df[\"EstimatedSalary\"] >= 150000\n","\n","    # NumOfProducts\n","    df[\"num_of_products_3_or_4\"] = df[\"NumOfProducts\"] >= 3\n","\n","    # Age\n","    df[\"age_over_40\"] = df[\"Age\"] >= 40\n","    df[\"age_over_50\"] = df[\"Age\"] >= 50\n","    df[\"age_over_60\"] = df[\"Age\"] >= 60\n","\n","    new_features = [\n","        \"balance_over_100k\",\n","        \"balance_over_150k\",\n","        \"estimated_salary_under_50k\",\n","        \"estimated_salary_50k_to_100k\",\n","        \"estamated_salary_over_150k\",\n","        \"num_of_products_3_or_4\",\n","        \"age_over_40\",\n","        \"age_over_50\",\n","        \"age_over_60\",\n","    ]\n","    for f in new_features:\n","        df[f] = df[f].astype(\"int\")\n","\n","    return df\n","\n","train_df = initial_feature_engineering(train_df)\n","train_df = feature_engineering_1(train_df)\n","X_train, X_val, y_train, y_val = train_test_split(train_df.drop(columns=target_col), train_df[target_col], test_size=0.2, random_state=RANDOM_SEED, stratify=train_df[target_col])"]},{"cell_type":"markdown","metadata":{},"source":["## Ideas for feature engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:04:03.311250Z","iopub.status.busy":"2024-01-04T20:04:03.310809Z","iopub.status.idle":"2024-01-04T20:04:03.330270Z","shell.execute_reply":"2024-01-04T20:04:03.328813Z","shell.execute_reply.started":"2024-01-04T20:04:03.311211Z"},"trusted":true},"outputs":[],"source":["def create_pipeline(model, numeric_scalers=(\"scaler\", StandardScaler())):\n","    numeric_pipeline = Pipeline(\n","        [numeric_scalers]\n","    )\n","\n","    categorical_pipeline = Pipeline([\n","        #(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n","        (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop='if_binary')),\n","    ])\n","\n","    preprocessor = ColumnTransformer([\n","        (\"numeric\", numeric_pipeline, numeric_features),\n","        #(\"categorical\", categorical_pipeline, categorical_features),\n","    ], remainder='passthrough')\n","\n","    return Pipeline([\n","        (\"preprocessor\", preprocessor),\n","        (\"classifier\", model),\n","    ])\n","\n","def train_models(models, X_train, y_train, parameters={}):\n","    trained_models = {}\n","    for model_name, model in tqdm(models.items()):\n","        if model_name in parameters:\n","            model.set_params(**parameters[model_name])\n","        model = create_pipeline(model)\n","        model.fit(X_train, y_train)\n","        trained_models[model_name] = model\n","    return trained_models\n","\n","def evaluate_models(models, X_val, y_val):\n","    # create a dataframe with \"model_name\", \"accuracy\", \"precision\", \"recall\", \"area under the ROC curve\"\n","    results_df = pd.DataFrame(columns=[\"model_name\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n","\n","    for model_name, model in tqdm(models.items()):\n","        y_pred = model.predict(X_val)\n","        y_proba = model.predict_proba(X_val)[:, 1]\n","        results_df = pd.concat([\n","            results_df,\n","            pd.DataFrame({\n","                \"model_name\": [model_name],\n","                \"accuracy\": [model.score(X_val, y_val)],\n","                \"precision\": [sklearn.metrics.precision_score(y_val, y_pred)],\n","                \"recall\": [sklearn.metrics.recall_score(y_val, y_pred)],\n","                \"auc\": [sklearn.metrics.roc_auc_score(y_val, y_proba)],\n","            })\n","        ])\n","    return results_df\n","\n","def plot_roc_curve(models, X_val, y_val):\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    palette_to_use = sns.color_palette(\"husl\", len(models))\n","    # for each model, plot the roc curve in the same plot, with other color\n","    for i, (model_name, model) in enumerate(models.items()):\n","        y_proba = model.predict_proba(X_val)[:, 1]\n","        fpr, tpr, _ = roc_curve(y_val, y_proba)\n","        roc_auc = roc_auc_score(y_val, y_proba)\n","        ax.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.2f})\", color=palette_to_use[i])\n","        ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n","    ax.set_xlabel(\"False Positive Rate\")\n","    ax.set_ylabel(\"True Positive Rate\")\n","    ax.set_title(\"ROC Curve\")\n","    # show legend\n","    ax.legend()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:09:02.848675Z","iopub.status.busy":"2024-01-04T20:09:02.848177Z","iopub.status.idle":"2024-01-04T20:09:02.915864Z","shell.execute_reply":"2024-01-04T20:09:02.914115Z","shell.execute_reply.started":"2024-01-04T20:09:02.848634Z"},"trusted":true},"outputs":[],"source":["trained_models = {}\n","\n","# generate random seed\n","models = {\n","    \"xgboost\": xgb.XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n","    \"lightgbm\": lgb.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, verbosity=-1),\n","    \"catboost\": cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False, thread_count=16),\n","    #\"knn\": KNeighborsClassifier(n_jobs=-1),\n","    #\"random_forest\": RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n","    #\"gradient_boosting\": GradientBoostingClassifier(random_state=RANDOM_SEED),\n","    #\"extra_trees\": ExtraTreesClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n","    #\"bagging\": BaggingClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n","    #\"ada_boost\": AdaBoostClassifier(random_state=RANDOM_SEED),\n","    #\"sgd\": SGDClassifier(random_state=RANDOM_SEED, loss=\"log_loss\", n_jobs=-1),\n","}\n","\n","print(\"Training models...\")\n","trained_models = train_models(models, X_train, y_train, trained_models)\n","print(\"Evaluating models...\")\n","results_df = evaluate_models(trained_models, X_val, y_val)\n","results_df.sort_values(by=\"auc\", ascending=False)"]},{"cell_type":"markdown","metadata":{},"source":["Comparison of ROC curves for trained models:"]},{"cell_type":"markdown","metadata":{},"source":["## Optimizing hyperparameters for the best model\n","\n","We will use optuna here to first optimize all different hyperparameters"]},{"cell_type":"markdown","metadata":{},"source":["## LightGBM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import optuna.integration.lightgbm as lgb\n","\n","from lightgbm import early_stopping\n","from lightgbm import log_evaluation\n","import sklearn.datasets\n","from sklearn.model_selection import StratifiedKFold\n","\n","\n","dtrain = lgb.Dataset(X_train, label=y_train)\n","\n","params = {\n","    \"objective\": \"binary\",\n","    \"num_class\": 1,\n","    \"is_unbalance\": \"true\",\n","    \"metric\": \"auc\",\n","    \"verbosity\": -1,\n","    \"boosting_type\": \"gbdt\",\n","}\n","\n","tuner = lgb.LightGBMTunerCV(\n","    params,\n","    dtrain,\n","    folds=StratifiedKFold(n_splits=3),\n","    callbacks=[early_stopping(1000), log_evaluation(100)],\n",")\n","\n","tuner.run()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","print(\"Best score:\", tuner.best_score)\n","lightgbm_best_params = tuner.best_params\n","print(\"Best params:\", lightgbm_best_params)\n","print(\"  Params: \")\n","for key, value in lightgbm_best_params.items():\n","    print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","metadata":{},"source":["## Xgboost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def objective(trial):\n","    dtrain = xgb.DMatrix(X_train, label=y_train, nthread=-1)\n","    param = {\n","        \"verbosity\": 0,\n","        \"objective\": \"binary:logistic\",\n","        \"eval_metric\": \"auc\",\n","        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n","        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n","        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 1000),\n","        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n","        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 0, 10),\n","        # on gpu\n","        \"device\": \"cuda\",\n","        \"tree_method\": \"hist\",\n","    }\n","\n","    if param[\"booster\"] == \"gbtree\" or param[\"booster\"] == \"dart\":\n","        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n","        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n","        param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n","        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n","    if param[\"booster\"] == \"dart\":\n","        param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n","        param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n","        param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n","        param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n","\n","    pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"test-auc\")\n","    history = xgb.cv(param, dtrain, num_boost_round=100, callbacks=[pruning_callback])\n","\n","    mean_auc = history[\"test-auc-mean\"].values[-1]\n","    return mean_auc\n","\n","pruner = optuna.pruners.MedianPruner(n_warmup_steps=5)\n","study = optuna.create_study(pruner=pruner, direction=\"maximize\")\n","study.optimize(objective, n_trials=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of finished trials: {}\".format(len(study.trials)))\n","print(\"Best trial:\")\n","xgboost_best_trial = study.best_trial\n","print(\"  Value: {}\".format(xgboost_best_trial.value))\n","xgboost_best_params = xgboost_best_trial.params\n","print(\"  Params: \")\n","for key, value in xgboost_best_params.items():\n","    print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","metadata":{},"source":["## Catboost"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from optuna.integration import CatBoostPruningCallback\n","from sklearn.metrics import roc_auc_score\n","def objective(trial: optuna.Trial) -> float:\n","    train_x, valid_x, train_y, valid_y = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train)\n","\n","    param = {\n","        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n","        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1, log=True),\n","        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n","        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n","        \"bootstrap_type\": trial.suggest_categorical(\n","            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n","        ),\n","        \"eval_metric\": \"AUC\",\n","    }\n","\n","    if param[\"bootstrap_type\"] == \"Bayesian\":\n","        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n","    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n","        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1, log=True)\n","\n","    gbm = cb.CatBoostClassifier(**param)\n","\n","    pruning_callback = CatBoostPruningCallback(trial, \"AUC\")\n","    gbm.fit(\n","        train_x,\n","        train_y,\n","        eval_set=[(valid_x, valid_y)],\n","        verbose=0,\n","        early_stopping_rounds=100,\n","        callbacks=[pruning_callback],\n","    )\n","\n","    # evoke pruning manually.\n","    pruning_callback.check_pruned()\n","    preds = gbm.predict_proba(valid_x)[:, 1]\n","    auc = roc_auc_score(valid_y, preds)\n","\n","    return auc\n","\n","study = optuna.create_study(\n","    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5), direction=\"maximize\"\n",")\n","study.optimize(objective, n_trials=100, timeout=600)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Number of finished trials: {}\".format(len(study.trials)))\n","\n","print(\"Best trial:\")\n","catboost_best_trial = study.best_trial\n","\n","print(\"  Value: {}\".format(catboost_best_trial.value))\n","catboost_best_params = catboost_best_trial.params\n","print(\"  Params: \")\n","for key, value in catboost_best_params.items():\n","    print(\"    {}: {}\".format(key, value))"]},{"cell_type":"markdown","metadata":{},"source":["## Best params"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["best_params_lightgbm = lightgbm_best_params\n","best_params_xgboost = xgboost_best_params\n","best_params_catboost = catboost_best_params\n","\n","optuna_best_parameters_found = {\n","    \"xgboost\": xgboost_best_params,\n","    \"lightgbm\": lightgbm_best_params,\n","    \"catboost\": catboost_best_params,\n","}\n","\n","optuna_best_parameters_found"]},{"cell_type":"markdown","metadata":{},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import lightgbm\n","\n","models = {\n","    \"xgboost\": xgb.XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n","    \"lightgbm\": lightgbm.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, verbosity=-1),\n","    \"catboost\": cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False),\n","    #\"logistic_regression\": LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1),\n","    \"knn\": KNeighborsClassifier(n_jobs=-1),\n","    \"stacked\": StackingClassifier(\n","        [\n","            (\"xgboost\", xgb.XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1, **xgboost_best_params)),\n","            (\"lightgbm\", lightgbm.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, **lightgbm_best_params)),\n","            (\"catboost\", cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False, **catboost_best_params)),\n","        ],\n","        final_estimator=LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1),\n","        n_jobs=-1,\n","    ),\n","}\n","\n","trained_models = train_models(models, X_train, y_train, parameters=optuna_best_parameters_found)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df = evaluate_models(trained_models, X_val, y_val)\n","results_df.sort_values(by=\"auc\", ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_roc_curve(trained_models, X_val, y_val)"]},{"cell_type":"markdown","metadata":{},"source":["# Submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_df = pd.read_csv(input_folder + \"playground-series-s4e1/train.csv\", index_col=\"id\")\n","test_df = pd.read_csv(input_folder + \"playground-series-s4e1/test.csv\", index_col=\"id\")\n","submission_df = pd.read_csv(input_folder + \"playground-series-s4e1/sample_submission.csv\")\n","original_df = pd.read_csv(input_folder + \"bank-customer-churn-prediction/Churn_Modelling.csv\")\n","target_col = \"Exited\"\n","\n","numeric_features = ['CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n","categorical_features = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n","\n","features_to_drop = ['CustomerId', 'Surname']\n","\n","GENERATED_COLUMN = True\n","ADD_ORIGINAL_DF = True\n","\n","model_postfix = \"_with_original\" if ADD_ORIGINAL_DF else \"\"\n","model_postfix += \"_generated\" if GENERATED_COLUMN else \"\"\n","\n","original_df = original_df.drop(columns=['RowNumber'])\n","\n","# drop na rows from orignal df\n","original_df = original_df.dropna()\n","\n","if GENERATED_COLUMN:\n","    train_df['generated'] = 1\n","    test_df['generated'] = 1\n","    original_df['generated'] = 0\n","    categorical_features.append('generated')\n","    \n","if ADD_ORIGINAL_DF:\n","    train_df = pd.concat([train_df, original_df])\n","\n","\n","for f in features_to_drop:\n","    if f in numeric_features:\n","        numeric_features.remove(f)\n","    if f in categorical_features:\n","        categorical_features.remove(f)\n","    \n","    train_df = train_df.drop(columns=f)\n","    test_df = test_df.drop(columns=f)\n","\n","train_df = initial_feature_engineering(train_df)\n","train_df = feature_engineering_1(train_df)\n","\n","test_df = initial_feature_engineering(test_df)\n","test_df = feature_engineering_1(test_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:09:37.302130Z","iopub.status.busy":"2024-01-04T20:09:37.301584Z","iopub.status.idle":"2024-01-04T20:09:50.619255Z","shell.execute_reply":"2024-01-04T20:09:50.617914Z","shell.execute_reply.started":"2024-01-04T20:09:37.302089Z"},"trusted":true},"outputs":[],"source":["# train model on train data\n","model = StackingClassifier(\n","    [\n","        (\"xgboost\", xgb.XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1, **xgboost_best_params)),\n","        (\"lightgbm\", lightgbm.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, **lightgbm_best_params)),\n","        (\"catboost\", cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False, **catboost_best_params)),\n","    ],\n","    final_estimator=LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1),\n","    n_jobs=-1,\n",")\n","model = cb.CatBoostClassifier(random_state=RANDOM_SEED, verbose=False, **catboost_best_params)\n","model = create_pipeline(model)\n","X_train = train_df.drop(columns=target_col)\n","y_train = train_df[target_col]\n","X_test = test_df\n","\n","model.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["Use for comparison or blending with other predictions:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-04T20:09:58.484835Z","iopub.status.busy":"2024-01-04T20:09:58.483806Z","iopub.status.idle":"2024-01-04T20:10:01.900339Z","shell.execute_reply":"2024-01-04T20:10:01.898976Z","shell.execute_reply.started":"2024-01-04T20:09:58.484778Z"},"trusted":true},"outputs":[],"source":["# predict on test data\n","y_pred = model.predict(X_test)\n","y_proba = model.predict_proba(X_test)[:, 1]\n","\n","\n","# create submission df\n","\n","submission_df = pd.DataFrame({\n","    \"id\": test_df.index,\n","    target_col: y_proba\n","})\n","submission_df.to_csv(\"./submission.csv\", index=False)\n","submission_df"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7405009,"sourceId":65711,"sourceType":"competition"},{"datasetId":3191230,"sourceId":5536933,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":4}
