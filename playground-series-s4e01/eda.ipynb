{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# âš½S4E1 - EDA & initial submission - Binary Classification with a Bank Churn Dataset \n",
    "\n",
    "Welcome to 2024! For this Episode of the Series, your task is to predict whether a customer continues with their account or closes it (e.g., churns). Good luck!\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n",
    "\n",
    "## Submission Format\n",
    "\n",
    "For each id in the test set, you must predict the probability for the target variable Exited. The file should contain a header and have the following format:\n",
    "\n",
    "```\n",
    "id,Exited\n",
    "0,0.9\n",
    "1,0.1\n",
    "2,0.5\n",
    "etc.\n",
    "```\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The dataset for this competition (both train and test) was generated from a deep learning model trained on the Bank Customer Churn Prediction dataset. Feature distributions are close to, but not exactly the same, as the original. \n",
    "\n",
    "# Code\n",
    "\n",
    "## ToC\n",
    "\n",
    "- [Imports](#Imports)\n",
    "\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentials\n",
    "import os\n",
    "import pathlib\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# visualisation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# sklearn imports\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MaxAbsScaler, PowerTransformer, FunctionTransformer, StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, MaxAbsScaler, PowerTransformer, FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, make_union, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif, SequentialFeatureSelector, RFECV\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.base import clone as clone_model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, log_loss\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator, KNNImputer\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, ElasticNet, SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier, TweedieRegressor\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB, ComplementNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import RocCurveDisplay, roc_auc_score, make_scorer, roc_curve\n",
    "\n",
    "from sklearn.preprocessing import Binarizer, Normalizer, RobustScaler, StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# others\n",
    "import xgboost as xgb \n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "import shap\n",
    "\n",
    "RANDOM_SEED = 64\n",
    "\n",
    "palette = [\"#4464ad\", \"#dc136c\", \"#F4FF52\", \"#f58f29\",\"#45cb85\"]\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(palette)\n",
    "sns.palplot(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading & EDA\n",
    "\n",
    "First we will check\n",
    "\n",
    "1. Number and types of columns\n",
    "2. Number of rows in train and test\n",
    "2. Missing values\n",
    "3. Target variable distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_KAGGLE = False\n",
    "\n",
    "kaggle_folder = \"/kaggle/input/\"\n",
    "local_folder = \"./data/\"\n",
    "input_folder = kaggle_folder if IN_KAGGLE else local_folder\n",
    "train_df = pd.read_csv(input_folder + \"playground-series-s4e1/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(input_folder + \"playground-series-s4e1/test.csv\", index_col=\"id\")\n",
    "submission_df = pd.read_csv(input_folder + \"playground-series-s4e1/sample_submission.csv\")\n",
    "original_df = pd.read_csv(input_folder + \"bank-customer-churn-prediction/Churn_Modelling.csv\")\n",
    "\n",
    "target_col = \"Exited\"\n",
    "\n",
    "def initial_feature_engineering(df):\n",
    "    df['HasCrCard'] = df['HasCrCard'].astype('bool')\n",
    "    df['IsActiveMember'] = df['IsActiveMember'].astype('bool')\n",
    "    return df\n",
    "\n",
    "train_df = initial_feature_engineering(train_df)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = len(train_df.columns)\n",
    "num_rows = len(train_df)\n",
    "\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div style=\"background-color: #4F8EC9; padding: 10px; border-radius: 5px; border: 5px solid #3C6D9C; margin: 10px 0;\">\n",
    "        <h4>Data shape</h4>\n",
    "        <p>Data contains 13 columns, of which 8 are numeric, 2 are boolean and 3 are categorical. There are no missing values.</p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of target column value counts across train and original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "fig.suptitle(\"Target distribution\", fontsize=20)\n",
    "\n",
    "sns.countplot(x=target_col, data=train_df, hue=target_col, ax=ax[0])\n",
    "sns.countplot(x=target_col, data=original_df, hue=target_col, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_train = len(train_df)\n",
    "total_original = len(original_df)\n",
    "\n",
    "\n",
    "train_target_pct = train_df[target_col].value_counts() / total_train\n",
    "original_target_pct = original_df[target_col].value_counts() / total_original\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "fig.suptitle(\"Target distribution (%)\", fontsize=20)\n",
    "\n",
    "sns.barplot(x=train_target_pct.index, y=train_target_pct.values, ax=ax[0], hue=train_target_pct.values, palette=palette)\n",
    "sns.barplot(x=original_target_pct.index, y=original_target_pct.values, ax=ax[1], hue=original_target_pct.values, palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition dataset and original have similar target distribution - around 80% of customers stay with the bank in each.\n",
    "\n",
    "\n",
    "Next steps:\n",
    "\n",
    "4. Distribution of numeric features\n",
    "5. Distribution of categorical features\n",
    "6. Correlation between numeric features and target\n",
    "7. Chi-square test on categorical features and target\n",
    "\n",
    "## Distribution of numeric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "# remove target col from list\n",
    "numeric_features.remove(target_col)\n",
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CustomerId - 'unique' identifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['CustomerId'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cnt = train_df['CustomerId'].value_counts().reset_index()\n",
    "val_cnt[ val_cnt['count'] > 1 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CreditScore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "fig.suptitle(f\"CreditScore histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"CreditScore\", data=train_df[train_df[target_col] == 0], ax=ax[0], kde=True, color=palette[0])\n",
    "sns.histplot(x=\"CreditScore\", data=train_df[train_df[target_col] == 1], ax=ax[1], kde=True, color=palette[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div style=\"background-color: #4F8EC9; padding: 10px; border-radius: 5px; border: 5px solid #3C6D9C; margin: 10px 0;\">\n",
    "        <h4>Credit score</h4>\n",
    "        <p>\n",
    "          <ul>\n",
    "            <li>Distribution has slight left skew, and we see the 850 cutoff as it's the max FICO credit score.</li>\n",
    "            <li>Min value is 350, max is 850, 50% of customers have score of at least 659. FICO scores could be as low as 300, but we don't have such customers in the dataset.</li>\n",
    "            <li>Credit score is not very different between customers who stay and leave</li>\n",
    "          </ul>        \n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Age histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"Age\", data=train_df[train_df[target_col] == 0], ax=ax[0], kde=True, color=palette[0])\n",
    "sns.histplot(x=\"Age\", data=train_df[train_df[target_col] == 1], ax=ax[1], kde=True, color=palette[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min Age is 18 while the oldest customer is 92 years old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tenure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tenure'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Tenure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_less_than_1_yr = train_df[train_df['Tenure'] < 1]['CustomerId'].unique()\n",
    "customers_more_than_9_yrs = train_df[train_df['Tenure'] > 9]['CustomerId'].unique()\n",
    "total_cust = len(train_df)\n",
    "print(f\"Customers with less than 1 year of tenure: {len(customers_less_than_1_yr)/total_cust*100:.2f}%\")\n",
    "print(f\"Customers with more than 9 years of tenure: {len(customers_more_than_9_yrs)/total_cust*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Tenure histogram\", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"Tenure\", data=train_df, ax=ax, kde=True, color=palette[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Tenure histogram\", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"Tenure\", data=train_df, ax=ax, kde=True, hue=target_col, color=palette[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 2% of customers have been with the bank for more than 9 years. Around the same number of customers came to bank less than 1 year ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Balance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 0]['Balance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 1]['Balance'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Balance histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "# distribution plot \n",
    "\n",
    "sns.histplot(x=\"Balance\", data=train_df[train_df[target_col] == 0], ax=ax[0], kde=True, color=palette[0])\n",
    "sns.histplot(x=\"Balance\", data=train_df[train_df[target_col] == 1], ax=ax[1], kde=True, color=palette[1])\n",
    "\n",
    "sns.displot(train_df, x=\"Balance\", hue=target_col, kind=\"ecdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div style=\"background-color: #4F8EC9; padding: 10px; border-radius: 5px; border: 5px solid #3C6D9C; margin: 10px 0;\">\n",
    "        <h4>Balance</h4>\n",
    "        <p>\n",
    "          <ul>\n",
    "            <li>More than 50% of customers balance was 0 (at the time of data collection). Max balance is around 250k.</li>\n",
    "            <li>Among customers who exited the bank, 50% of them had a balance of at least ~100k</li>\n",
    "          </ul>        \n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NumOfProducts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['NumOfProducts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 0]['NumOfProducts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 0]['NumOfProducts'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"NumOfProducts histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"NumOfProducts\", data=train_df[train_df[target_col] == 0], ax=ax[0], kde=True, color=palette[0])\n",
    "sns.histplot(x=\"NumOfProducts\", data=train_df[train_df[target_col] == 1], ax=ax[1], kde=True, color=palette[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"NumOfProducts histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"NumOfProducts\", data=train_df, ax=ax, kde=True, hue=target_col, color=palette[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_or_more_products = train_df[train_df['NumOfProducts'] >= 3]\n",
    "total_num_three_or_more_products = len(three_or_more_products)\n",
    "val_cnt = three_or_more_products[target_col].value_counts()\n",
    "# calculate pct\n",
    "val_cnt = val_cnt / total_num_three_or_more_products * 100\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "less_than_three = train_df[train_df['NumOfProducts'] < 3]\n",
    "less_than_three[target_col].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div style=\"background-color: #4F8EC9; padding: 10px; border-radius: 5px; border: 5px solid #3C6D9C; margin: 10px 0;\">\n",
    "        <h4>NumOfProducts</h4>\n",
    "        <p>\n",
    "          <ul>\n",
    "            <li>In both groups, min number of products was 1 while max was 4</li>\n",
    "            <li>Among customers who stay with bank, most of them have 2 products, while having 1 product is the second most common</li>\n",
    "            <li>Among customers who exited the bank, having 1 product is the most common, but having 3 or 4 products is much more common than in the other group</li>\n",
    "            <li>Almost 90% of customers who have 3 or 4 products leave the bank</li>\n",
    "          </ul>        \n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EstimatedSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"EstimatedSalary\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 0][\"EstimatedSalary\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[train_df[target_col] == 1][\"EstimatedSalary\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"EstimatedSalary histogram ({target_col} = 0, {target_col} = 1) \", fontsize=20)\n",
    "\n",
    "sns.histplot(x=\"EstimatedSalary\", data=train_df[train_df[target_col] == 0], ax=ax[0], kde=True, color=palette[0])\n",
    "sns.histplot(x=\"EstimatedSalary\", data=train_df[train_df[target_col] == 1], ax=ax[1], kde=True, color=palette[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People who leave the bank have a slightly higher salary than those who stay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train_df.select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HasCrCard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"HasCrCard distribution between target groups\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'HasCrCard']].value_counts(normalize=True).reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='HasCrCard', columns=target_col, values='proportion', aggfunc=sum, fill_value=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"HasCrCard distribution between target groups\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'HasCrCard']].value_counts().reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='HasCrCard', columns=target_col, values='count', aggfunc=sum, fill_value=0)\n",
    "# divide each cell by total sum in each column\n",
    "pivot_table = pivot_table / pivot_table.sum(axis=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IsActiveMember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"IsActiveMember distribution between target groups\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'IsActiveMember']].value_counts(normalize=True).reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='IsActiveMember', columns=target_col, values='proportion', aggfunc=sum, fill_value=0)\n",
    "\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"IsActiveMember distribution between target groups (proportions among target groups)\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'IsActiveMember']].value_counts().reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='IsActiveMember', columns=target_col, values='count', aggfunc=sum, fill_value=0)\n",
    "# divide each cell by total sum in each column\n",
    "pivot_table = pivot_table / pivot_table.sum(axis=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <div style=\"background-color: #4F8EC9; padding: 10px; border-radius: 5px; border: 5px solid #3C6D9C; margin: 10px 0;\">\n",
    "        <h4>HasCrCard & IsActiveMember</h4>\n",
    "        <p>\n",
    "          <ul>\n",
    "            <li>Having a credit card does not seem to contribute to staying or leaving.</li>\n",
    "            <li>70% of people who left were not active members while only 55% of people who stayed were active members.</li>\n",
    "          </ul>        \n",
    "        </p>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[[target_col, 'Gender']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Gender proportions among target groups\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'Gender']].value_counts().reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='Gender', columns=target_col, values='count', aggfunc=sum, fill_value=0)\n",
    "# divide each cell by total sum in each column\n",
    "pivot_table = pivot_table / pivot_table.sum(axis=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among people who stay with bank, 60% are men, while 60% of people who leave the bank are women."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "\n",
    "sns.despine()\n",
    "fig.suptitle(f\"Geography proportions among target groups\", fontsize=20)\n",
    "\n",
    "val_cnt = train_df[[target_col, 'Geography']].value_counts().reset_index()\n",
    "pivot_table = val_cnt.pivot_table(index='Geography', columns=target_col, values='count', aggfunc=sum, fill_value=0)\n",
    "# divide each cell by total sum in each column\n",
    "pivot_table = pivot_table / pivot_table.sum(axis=0)\n",
    "sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"Reds\")\n",
    "val_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Surname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Surname'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not likely to be useful for prediction. We will drop this column before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation between numeric features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlation matrix\n",
    "\n",
    "corr_matrix = train_df[numeric_features + [target_col]].corr( method='pearson')\n",
    "\n",
    "\n",
    "fig.suptitle(\"Correlation matrix\", fontsize=20)\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"Reds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not corellated features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-square test on categorical features and target\n",
    "\n",
    "Here we want to create a table which could tell us by which features the target groups differ the most. We will use chi-square test to check if the difference is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "def chi_square_test(dataframe, target_col, categorical_features):\n",
    "    # Create a dictionary to hold the results\n",
    "    results = {\"Feature\": [], \"Chi-Square Statistic\": [], \"p-Value\": [], \"Significant\": []}\n",
    "\n",
    "    # Split the data into two groups based on the target column\n",
    "    group_1 = dataframe[dataframe[target_col] == 0]\n",
    "    group_2 = dataframe[dataframe[target_col] == 1]\n",
    "\n",
    "    # Iterate over each categorical feature\n",
    "    for feature in categorical_features:\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(dataframe[feature], dataframe[target_col])\n",
    "\n",
    "        # Perform the chi-square test\n",
    "        chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "        # Determine if the result is significant\n",
    "        significant = p < 0.005\n",
    "\n",
    "        # Append the results to the dictionary\n",
    "        results[\"Feature\"].append(feature)\n",
    "        results[\"Chi-Square Statistic\"].append(chi2)\n",
    "        results[\"p-Value\"].append(p)\n",
    "        results[\"Significant\"].append(significant)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame and return it\n",
    "    return pd.DataFrame(results).sort_values(by=\"Chi-Square Statistic\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = chi_square_test(train_df, target_col, [\"Geography\", \"IsActiveMember\", \"Gender\", \"HasCrCard\"])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(input_folder + \"playground-series-s4e1/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(input_folder + \"playground-series-s4e1/test.csv\", index_col=\"id\")\n",
    "submission_df = pd.read_csv(input_folder + \"playground-series-s4e1/sample_submission.csv\")\n",
    "original_df = pd.read_csv(input_folder + \"bank-customer-churn-prediction/Churn_Modelling.csv\")\n",
    "target_col = \"Exited\"\n",
    "\n",
    "numeric_features = ['CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "categorical_features = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
    "\n",
    "features_to_drop = ['CustomerId', 'Surname']\n",
    "\n",
    "for f in features_to_drop:\n",
    "    if f in numeric_features:\n",
    "        numeric_features.remove(f)\n",
    "    if f in categorical_features:\n",
    "        categorical_features.remove(f)\n",
    "    \n",
    "    train_df = train_df.drop(columns=f)\n",
    "\n",
    "def initial_feature_engineering(df):\n",
    "    df['HasCrCard'] = df['HasCrCard'].astype('bool')\n",
    "    df['IsActiveMember'] = df['IsActiveMember'].astype('bool')\n",
    "    df['Gender'] = df['Gender'].map({ \"Male\": 0, \"Female\": 1}).astype(\"bool\")\n",
    "    # encode geography\n",
    "    df = pd.get_dummies(df, columns=['Geography'])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = initial_feature_engineering(train_df)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_df.drop(columns=target_col), train_df[target_col], test_size=0.2, random_state=RANDOM_SEED, stratify=train_df[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_pipeline(model, numeric_scalers=(\"scaler\", StandardScaler())):\n",
    "    numeric_pipeline = Pipeline(\n",
    "        [numeric_scalers]\n",
    "    )\n",
    "\n",
    "    categorical_pipeline = Pipeline([\n",
    "        #(\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"one_hot_encoder\", OneHotEncoder(handle_unknown=\"ignore\", drop='if_binary')),\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        (\"numeric\", numeric_pipeline, numeric_features),\n",
    "        #(\"categorical\", categorical_pipeline, categorical_features),\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "    return Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"classifier\", model),\n",
    "    ])\n",
    "\n",
    "def train_models(models, X_train, y_train):\n",
    "    trained_models = {}\n",
    "    for model_name, model in tqdm(models.items()):\n",
    "        model = create_pipeline(model)\n",
    "        model.fit(X_train, y_train)\n",
    "        trained_models[model_name] = model\n",
    "    return trained_models\n",
    "\n",
    "def evaluate_models(models, X_val, y_val):\n",
    "    # create a dataframe with \"model_name\", \"accuracy\", \"precision\", \"recall\", \"area under the ROC curve\"\n",
    "    results_df = pd.DataFrame(columns=[\"model_name\", \"accuracy\", \"precision\", \"recall\", \"auc\"])\n",
    "\n",
    "    for model_name, model in tqdm(models.items()):\n",
    "        y_pred = model.predict(X_val)\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        results_df = pd.concat([\n",
    "            results_df,\n",
    "            pd.DataFrame({\n",
    "                \"model_name\": [model_name],\n",
    "                \"accuracy\": [model.score(X_val, y_val)],\n",
    "                \"precision\": [sklearn.metrics.precision_score(y_val, y_pred)],\n",
    "                \"recall\": [sklearn.metrics.recall_score(y_val, y_pred)],\n",
    "                \"auc\": [sklearn.metrics.roc_auc_score(y_val, y_proba)],\n",
    "            })\n",
    "        ])\n",
    "    return results_df\n",
    "\n",
    "def plot_roc_curve(models, X_val, y_val):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "    palette_to_use = sns.color_palette(\"husl\", len(models))\n",
    "    # for each model, plot the roc curve in the same plot, with other color\n",
    "    for i, (model_name, model) in enumerate(models.items()):\n",
    "        y_proba = model.predict_proba(X_val)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_proba)\n",
    "        roc_auc = roc_auc_score(y_val, y_proba)\n",
    "        ax.plot(fpr, tpr, label=f\"{model_name} (AUC = {roc_auc:.2f})\", color=palette_to_use[i])\n",
    "        ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "    ax.set_xlabel(\"False Positive Rate\")\n",
    "    ax.set_ylabel(\"True Positive Rate\")\n",
    "    ax.set_title(\"ROC Curve\")\n",
    "    # show legend\n",
    "    ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"xgboost\": xgb.XGBClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"lightgbm\": lgb.LGBMClassifier(random_state=RANDOM_SEED, n_jobs=-1, verbosity=-1),\n",
    "    \"logistic_regression\": LogisticRegression(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"knn\": KNeighborsClassifier(n_jobs=-1),\n",
    "    \"decision_tree\": DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
    "    \"random_forest\": RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"gradient_boosting\": GradientBoostingClassifier(random_state=RANDOM_SEED),\n",
    "    \"extra_trees\": ExtraTreesClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"bagging\": BaggingClassifier(random_state=RANDOM_SEED, n_jobs=-1),\n",
    "    \"sgd\": SGDClassifier(random_state=RANDOM_SEED, loss=\"log_loss\", n_jobs=-1),\n",
    "}\n",
    "\n",
    "print(\"Training models...\")\n",
    "trained_models = train_models(models, X_train, y_train)\n",
    "print(\"Evaluating models...\")\n",
    "results_df = evaluate_models(trained_models, X_val, y_val)\n",
    "results_df.sort_values(by=\"auc\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(trained_models, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Shapley values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = trained_models['lightgbm'].named_steps[\"classifier\"]\n",
    "\n",
    "x_shap_sample = X_train.sample(500)\n",
    "\n",
    "explainer = shap.TreeExplainer(best_model, data=x_shap_sample, feature_perturbation=\"interventional\", model_output='probability')\n",
    "shap_values = explainer.shap_values(x_shap_sample)\n",
    "\n",
    "shap.summary_plot(shap_values, X_train, plot_type=\"violin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a particular sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_target_0 = x_shap_sample[ x_shap_sample[target_col] == 0 ].sample(1).iloc[0]\n",
    "sample_target_1 = x_shap_sample[ x_shap_sample[target_col] == 1 ].sample(1).iloc[0]\n",
    "\n",
    "\n",
    "shap.plots.waterfall(shap_values[sample_target_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[sample_target_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[sample_target_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[sample_target_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing hyperparameters for the best model\n",
    "\n",
    "We will use optuna here to first optimize all different hyperparameters, then second time to find best learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_lightgbm(trial):\n",
    "    lightgbm_model_optuna = lgb.LGBMClassifier(random_state=RANDOM_SEED, verbose=-1, n_jobs=-1)\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    clf = create_pipeline(lightgbm_model_optuna)\n",
    "    params = {\n",
    "        'classifier__n_estimators' : 377,\n",
    "        #\"classifier__learning_rate\" : trial.suggest_float('learning_rate',1e-4, 0.25, log=True),\n",
    "        \"classifier__max_depth\":trial.suggest_int('max_depth',3,50),\n",
    "        'classifier__reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n",
    "        'classifier__reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n",
    "        \"classifier__min_child_weight\" : trial.suggest_float('min_child_weight', 0.5,4),\n",
    "        \"classifier__min_child_samples\" : trial.suggest_int('min_child_samples',1,100),\n",
    "        \"classifier__subsample\" : trial.suggest_float('subsample', 0.4, 1),\n",
    "        \"classifier__subsample_freq\" : trial.suggest_int('subsample_freq',0,5),\n",
    "        \"classifier__colsample_bytree\" : trial.suggest_float('colsample_bytree',0.2,1),\n",
    "        \"classifier__num_leaves\" : trial.suggest_int('num_leaves', 2, 64*2),\n",
    "        \"classifier__max_bin\" : trial.suggest_int('max_bin', 128, 1024),\n",
    "    }\n",
    "    \n",
    "    clf.set_params(**params)\n",
    "    return cross_val_score(clf, X_train, y_train, cv = skf, scoring='roc_auc', n_jobs=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_lightgbm, n_trials=1, timeout=1000)\n",
    "print(\"Best score:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "lgbm_best_params = study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(input_folder + \"playground-series-s4e1/train.csv\", index_col=\"id\")\n",
    "test_df = pd.read_csv(input_folder + \"playground-series-s4e1/test.csv\", index_col=\"id\")\n",
    "submission_df = pd.read_csv(input_folder + \"playground-series-s4e1/sample_submission.csv\")\n",
    "original_df = pd.read_csv(input_folder + \"bank-customer-churn-prediction/Churn_Modelling.csv\")\n",
    "target_col = \"Exited\"\n",
    "\n",
    "numeric_features = ['CustomerId', 'CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "categorical_features = ['Surname', 'Geography', 'Gender', 'HasCrCard', 'IsActiveMember']\n",
    "\n",
    "features_to_drop = ['CustomerId', 'Surname']\n",
    "\n",
    "for f in features_to_drop:\n",
    "    if f in numeric_features:\n",
    "        numeric_features.remove(f)\n",
    "    if f in categorical_features:\n",
    "        categorical_features.remove(f)\n",
    "    \n",
    "    test_df = test_df.drop(columns=f)\n",
    "    train_df = train_df.drop(columns=f)\n",
    "\n",
    "def initial_feature_engineering(df):\n",
    "    df['HasCrCard'] = df['HasCrCard'].astype('bool')\n",
    "    df['IsActiveMember'] = df['IsActiveMember'].astype('bool')\n",
    "    df['Gender'] = df['Gender'].map({ \"Male\": 0, \"Female\": 1}).astype(\"bool\")\n",
    "    # encode geography\n",
    "    df = pd.get_dummies(df, columns=['Geography'])\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = initial_feature_engineering(train_df)\n",
    "test_df = initial_feature_engineering(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on train data\n",
    "model = clone_model(trained_models['lightgbm'])\n",
    "model.set_params(**lgbm_best_params)\n",
    "\n",
    "X_train = train_df.drop(columns=target_col)\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "\n",
    "# create submission df\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    \"id\": test_df.index,\n",
    "    target_col: y_proba\n",
    "})\n",
    "\n",
    "\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
