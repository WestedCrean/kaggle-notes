{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive approach random_forest + optuna - holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'ltsm_pytorch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>country</th>\n",
       "      <th>store</th>\n",
       "      <th>product</th>\n",
       "      <th>num_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Improve Your Coding</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Train More LLMs</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win Friends and Influence People</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Win More Kaggle Competitions</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Learn</td>\n",
       "      <td>Using LLMs to Write Better</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Store</td>\n",
       "      <td>Using LLMs to Improve Your Coding</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Store</td>\n",
       "      <td>Using LLMs to Train More LLMs</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Store</td>\n",
       "      <td>Using LLMs to Win Friends and Influence People</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Store</td>\n",
       "      <td>Using LLMs to Win More Kaggle Competitions</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>Kaggle Store</td>\n",
       "      <td>Using LLMs to Write Better</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date    country         store  \\\n",
       "0  2017-01-01  Argentina  Kaggle Learn   \n",
       "1  2017-01-01  Argentina  Kaggle Learn   \n",
       "2  2017-01-01  Argentina  Kaggle Learn   \n",
       "3  2017-01-01  Argentina  Kaggle Learn   \n",
       "4  2017-01-01  Argentina  Kaggle Learn   \n",
       "5  2017-01-01  Argentina  Kaggle Store   \n",
       "6  2017-01-01  Argentina  Kaggle Store   \n",
       "7  2017-01-01  Argentina  Kaggle Store   \n",
       "8  2017-01-01  Argentina  Kaggle Store   \n",
       "9  2017-01-01  Argentina  Kaggle Store   \n",
       "\n",
       "                                          product  num_sold  \n",
       "0               Using LLMs to Improve Your Coding        63  \n",
       "1                   Using LLMs to Train More LLMs        66  \n",
       "2  Using LLMs to Win Friends and Influence People         9  \n",
       "3      Using LLMs to Win More Kaggle Competitions        59  \n",
       "4                      Using LLMs to Write Better        49  \n",
       "5               Using LLMs to Improve Your Coding        88  \n",
       "6                   Using LLMs to Train More LLMs        98  \n",
       "7  Using LLMs to Win Friends and Influence People        14  \n",
       "8      Using LLMs to Win More Kaggle Competitions        83  \n",
       "9                      Using LLMs to Write Better        69  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data_path = \"./data/\"\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")\n",
    "\n",
    "# drop id\n",
    "df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "import requests\n",
    "import holidays\n",
    "\n",
    "# magical constants\n",
    "\n",
    "def add_magical_constants(df):\n",
    "    coef_c = {'Argentina': 4.23, 'Spain': 1.500, 'Japan': 1.14, 'Estonia': 1.62, 'Canada': 0.87}\n",
    "    for c, country in enumerate(df['country'].unique()):\n",
    "        df.loc[(df['country'] == country), 'num_sold'] *= coef_c[country]\n",
    "    return df\n",
    "\n",
    "# gdp feature\n",
    "\n",
    "def get_gdp_per_capita(country,year):\n",
    "    alpha3 = {'Argentina':'ARG','Canada':'CAN','Estonia':'EST','Japan':'JPN','Spain':'ESP'}\n",
    "    url=\"https://api.worldbank.org/v2/country/{0}/indicator/NY.GDP.PCAP.CD?date={1}&format=json\".format(alpha3[country],year)\n",
    "    response = requests.get(url).json()\n",
    "    return response[1][0]['value']\n",
    "\n",
    "def create_gdp_df(df):\n",
    "    gdp = []\n",
    "    for country in df.country.unique():\n",
    "        row = []\n",
    "        for year in range(2017,2023):\n",
    "            row.append(get_gdp_per_capita(country,year))\n",
    "        gdp.append(row)\n",
    "\n",
    "    gdp = np.array(gdp)\n",
    "    gdp /= np.sum(gdp,axis=0)\n",
    "\n",
    "    rel_gdp_df = pd.DataFrame(gdp,index=df.country.unique(),columns=range(2017,2023))\n",
    "    return rel_gdp_df\n",
    "\n",
    "def add_gdp_feature(df):\n",
    "    rel_gdp_df_0 = create_gdp_df(df)\n",
    "    rel_gdp_df = rel_gdp_df_0.reset_index(names=\"country\")\n",
    "    rel_gdp_df = pd.melt(rel_gdp_df, id_vars='country', value_vars=[2017, 2018, 2019, 2020, 2021, 2022])\n",
    "    rel_gdp_df.columns = ['country', 'year', 'rel_gdp']\n",
    "    rel_gdp_df['year'] = rel_gdp_df['year'].astype(int)\n",
    "    df = df.merge(rel_gdp_df, on=['year', 'country'], how='left')\n",
    "    return df\n",
    "\n",
    "# holidays feature\n",
    "\n",
    "def create_holidays_df():\n",
    "    years = [2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "    countries = {\n",
    "        \"AR\": \"Argentina\",\n",
    "        \"CA\": \"Canada\",\n",
    "        \"EE\": \"Estonia\",\n",
    "        \"ES\": \"Spain\",\n",
    "        \"JP\": \"Japan\"\n",
    "    }\n",
    "\n",
    "\n",
    "    Argentina_holidays = holidays.CountryHoliday('AR', years=years)\n",
    "    Canada_holidays = holidays.CountryHoliday('CA', years=years)\n",
    "    Estonia_holidays = holidays.CountryHoliday('EE', years=years)\n",
    "    Spain_holidays = holidays.CountryHoliday('ES', years=years)\n",
    "    Japan_holidays = holidays.CountryHoliday('JP', years=years)\n",
    "\n",
    "    all_holidays = [Argentina_holidays, Canada_holidays, Estonia_holidays, Spain_holidays, Japan_holidays]\n",
    "\n",
    "\n",
    "\n",
    "    holidays_dfs = []\n",
    "    for country_holidays in all_holidays:\n",
    "        holidays_dates = []\n",
    "        for date in country_holidays:\n",
    "            holidays_dates.append(date)\n",
    "\n",
    "        holidays_dates = list(set(holidays_dates))\n",
    "\n",
    "        hdf = pd.DataFrame(holidays_dates, columns=['date'])\n",
    "        hdf['country'] = countries[country_holidays.country]\n",
    "\n",
    "        holidays_dfs.append(hdf)\n",
    "\n",
    "    holidays_df = pd.concat(holidays_dfs)\n",
    "\n",
    "    holidays_df['is_holiday'] = True\n",
    "\n",
    "    holidays_df['date'] = pd.to_datetime(holidays_df['date'])\n",
    "    return holidays_df\n",
    "\n",
    "def add_holiday_feature(df):\n",
    "    holidays_df = create_holidays_df()\n",
    "    df = df.merge(holidays_df, on=['date', 'country'], how='left')\n",
    "    df['is_holiday'] = df['is_holiday'].fillna(False)\n",
    "    df['is_holiday'] = df['is_holiday'].astype(int)\n",
    "    return df\n",
    "\n",
    "\n",
    "def transform_date(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "def create_features(df):\n",
    "    # split date into year, month, day\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day'] = df['date'].dt.day\n",
    "\n",
    "    # week number\n",
    "    df['week_number'] = df['date'].dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # day of week\n",
    "    df['dayofweek'] = df['date'].dt.dayofweek\n",
    "    df['dayofmonth'] = df['date'].dt.day\n",
    "    # weekend\n",
    "    df['weekend'] = (df['date'].dt.weekday >=4).astype(int)\n",
    "\n",
    "    df['country'] = df['country'].astype('category')\n",
    "    df['store'] = df['store'].astype('category')\n",
    "    df['product'] = df['product'].astype('category')\n",
    "\n",
    "    # remove march 2020 - june 2020\n",
    "    #df = df[(df['date'] < '2020-03-01') | (df['date'] > '2020-06-30')]\n",
    "\n",
    "    return df\n",
    "\n",
    "def encode_cat_variables(df):\n",
    "    categorical_features = [\"country\", \"store\", \"product\"]\n",
    "    df = pd.get_dummies(df, columns=categorical_features)\n",
    "    return df\n",
    "\n",
    "def seasonality_features(df):\n",
    "    df['month_sin'] = np.sin(2*np.pi*df.month/12)\n",
    "    df['month_cos'] = np.cos(2*np.pi*df.month/12)\n",
    "    df['day_sin'] = np.sin(2*np.pi*df.dayofmonth/31)\n",
    "    df['day_cos'] = np.cos(2*np.pi*df.dayofmonth/31)\n",
    "    return df\n",
    "\n",
    "def SMAPE(A, F):\n",
    "    return 100/len(A) * np.sum(2 * np.abs(F - A) / (np.abs(A) + np.abs(F)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TimeSeriesDataSet' from 'torch.utils.data' (c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mimport\u001b[39;00m functional \u001b[39mas\u001b[39;00m F\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m nn\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorDataset, DataLoader, TimeSeriesDataSet\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, transforms\n\u001b[0;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlightning\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorch\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpl\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'TimeSeriesDataSet' from 'torch.utils.data' (c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torchmetrics.functional.regression import mean_squared_error\n",
    "\n",
    "class TimeseriesDataModule(pl.LightningDataModule):\n",
    "    '''\n",
    "    PyTorch Lighting DataModule subclass:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/datamodules.html\n",
    "\n",
    "    Serves the purpose of aggregating all data loading \n",
    "      and processing work in one place.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, seq_len = 1, batch_size = 128, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_val = None\n",
    "        self.y_val = None\n",
    "        self.X_test = None\n",
    "        self.X_test = None\n",
    "        self.columns = None\n",
    "        self.preprocessing = None\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        '''\n",
    "        Data is resampled to hourly intervals.\n",
    "        Both 'np.nan' and '?' are converted to 'np.nan'\n",
    "        'Date' and 'Time' columns are merged into 'dt' index\n",
    "        '''\n",
    "\n",
    "        if stage == 'fit' and self.X_train is not None:\n",
    "            return \n",
    "        if stage == 'test' and self.X_test is not None:\n",
    "            return\n",
    "        if stage is None and self.X_train is not None and self.X_test is not None:  \n",
    "            return\n",
    "\n",
    "        data_path = \"./data/\"\n",
    "        df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "        df_test = pd.read_csv(data_path + \"test.csv\")\n",
    "\n",
    "        # drop id\n",
    "        df_train.drop('id', axis=1, inplace=True)\n",
    "\n",
    "        df_train = transform_date(df_train)\n",
    "        df_train = create_features(df_train)\n",
    "        df_train = seasonality_features(df_train)\n",
    "        df_train = add_gdp_feature(df_train)\n",
    "        df_train = add_holiday_feature(df_train)\n",
    "        df_train = encode_cat_variables(df_train)\n",
    "\n",
    "\n",
    "        #df_test_0 = transform_date(df_test)\n",
    "        #df_test_0 = create_features(df_test_0)\n",
    "        #df_test_0 = seasonality_features(df_test_0)\n",
    "        #df_test_0 = add_gdp_feature(df_test_0)\n",
    "        #df_test_0 = add_holiday_feature(df_test_0)\n",
    "        #df_test_0 = encode_cat_variables(df_test_0)\n",
    "\n",
    "        model_features = df_train.columns.tolist()\n",
    "        model_features.remove(\"num_sold\")\n",
    "        model_features.remove(\"date\")\n",
    "\n",
    "        # we pick the last half year of 2021 as validation set\n",
    "\n",
    "        X_train = df_train[df_train[\"date\"] < \"2021-06-01\"][model_features]\n",
    "        y_train = df_train[df_train[\"date\"] < \"2021-06-01\"][\"num_sold\"]\n",
    "\n",
    "        X_val = df_train[df_train[\"date\"] >= \"2021-06-01\"][model_features]\n",
    "        y_val = df_train[df_train[\"date\"] >= \"2021-06-01\"][\"num_sold\"]\n",
    "\n",
    "        cat_features_indices = np.where((X_train.dtypes == \"category\") | (X_train.dtypes == \"object\"))[0]\n",
    "        cat_features_indices\n",
    "\n",
    "\n",
    "        preprocessing = StandardScaler()\n",
    "        preprocessing.fit(X_train)\n",
    "\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.X_train = preprocessing.transform(X_train)\n",
    "            self.y_train = y_train.values.reshape((-1, 1))\n",
    "            self.X_val = preprocessing.transform(X_val)\n",
    "            self.y_val = y_val.values.reshape((-1, 1))\n",
    "        \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = TimeSeriesDataSet(self.X_train, \n",
    "                                          self.y_train, \n",
    "                                          seq_len=self.seq_len)\n",
    "        train_loader = DataLoader(train_dataset, \n",
    "                                  batch_size = self.batch_size, \n",
    "                                  shuffle = False, \n",
    "                                  num_workers = self.num_workers)\n",
    "        \n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_dataset = TimeSeriesDataSet(self.X_val, \n",
    "                                        self.y_val, \n",
    "                                        seq_len=self.seq_len)\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size = self.batch_size, \n",
    "                                shuffle = False, \n",
    "                                num_workers = self.num_workers)\n",
    "\n",
    "        return val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegressor(pl.LightningModule):\n",
    "    '''\n",
    "    Standard PyTorch Lightning module:\n",
    "    https://pytorch-lightning.readthedocs.io/en/latest/lightning_module.html\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 n_features, \n",
    "                 hidden_size, \n",
    "                 seq_len, \n",
    "                 batch_size,\n",
    "                 num_layers, \n",
    "                 dropout, \n",
    "                 learning_rate,\n",
    "                 criterion):\n",
    "        super(LSTMRegressor, self).__init__()\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=n_features, \n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, \n",
    "                            dropout=dropout, \n",
    "                            batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # lstm_out = (batch_size, seq_len, hidden_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        y_pred = self.linear(lstm_out[:,-1])\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.TrainResult(loss)\n",
    "        result.log('train_loss', loss)\n",
    "        return result\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult(checkpoint_on=loss)\n",
    "        result.log('val_loss', loss)\n",
    "        return result\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        result = pl.EvalResult()\n",
    "        result.log('test_loss', loss)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    seq_len = 24,\n",
    "    batch_size = 70, \n",
    "    criterion = nn.MSELoss(),\n",
    "    max_epochs = 10,\n",
    "    n_features = 7,\n",
    "    hidden_size = 100,\n",
    "    num_layers = 1,\n",
    "    dropout = 0.2,\n",
    "    learning_rate = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Wiktor\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\lightning\\fabric\\connector.py:554: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'LSTMRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m logger \u001b[39m=\u001b[39m TensorBoardLogger(\u001b[39m\"\u001b[39m\u001b[39mlightning_logs\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mplayground-series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[0;32m     12\u001b[0m     max_epochs\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mmax_epochs\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m     13\u001b[0m     precision\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, \n\u001b[0;32m     14\u001b[0m     accelerator\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     15\u001b[0m     logger\u001b[39m=\u001b[39mlogger,\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 18\u001b[0m model \u001b[39m=\u001b[39m LSTMRegressor(\n\u001b[0;32m     19\u001b[0m     n_features \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mn_features\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     20\u001b[0m     hidden_size \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m     seq_len \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m     batch_size \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     23\u001b[0m     criterion \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mcriterion\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     24\u001b[0m     num_layers \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     25\u001b[0m     dropout \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     26\u001b[0m     learning_rate \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m dm \u001b[39m=\u001b[39m TimeseriesDataModule(\n\u001b[0;32m     30\u001b[0m     seq_len \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mseq_len\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     31\u001b[0m     batch_size \u001b[39m=\u001b[39m params[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, dm)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSTMRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"playground-series\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=params['max_epochs'], \n",
    "    precision=16, \n",
    "    accelerator=\"gpu\",\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "model = LSTMRegressor(\n",
    "    n_features = params['n_features'],\n",
    "    hidden_size = params['hidden_size'],\n",
    "    seq_len = params['seq_len'],\n",
    "    batch_size = params['batch_size'],\n",
    "    criterion = params['criterion'],\n",
    "    num_layers = params['num_layers'],\n",
    "    dropout = params['dropout'],\n",
    "    learning_rate = params['learning_rate']\n",
    ")\n",
    "\n",
    "dm = TimeseriesDataModule(\n",
    "    seq_len = params['seq_len'],\n",
    "    batch_size = params['batch_size']\n",
    ")\n",
    "\n",
    "trainer.fit(model, dm)\n",
    "#trainer.test(model, datamodule=dm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
